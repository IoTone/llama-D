/**
#
# Copyright (c) 2024 IoTone, Inc. All rights reserved.
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of this
# software and associated documentation files (the "Software"), to deal in the 
# Software without restriction, including without limitation the rights to use, 
# copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the 
# Software, and to permit persons to whom the Software is furnished to do so, subject
# to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A 
# PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT 
# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF 
# CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE
# OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
#
**/
//
// Generated by: gcc -E -P llama.h > llama.lst
//
module iotone.llamacpp;

import core.stdc.config;
import core.stdc.stdint;
import std.typecons;

// TODO: what about these includes
// #include "ggml.h"
// #include "ggml-backend.h"
struct __nt_context { int esp; int info; int prev; int handler; int stable; int sindex; int ebp; };




//
// GGML Tensor Library
//
// This documentation is still a work in progress.
// If you wish some specific topics to be covered, feel free to drop a comment:
//
//   https://github.com/ggerganov/whisper.cpp/issues/40
//
// ## Overview
//
// This library implements:
//
//  - a set of tensor operations
//  - automatic differentiation
//  - basic optimization algorithms
//
// The aim of this library is to provide a minimalistic approach for various machine learning tasks. This includes,
// but is not limited to, the following:
//
//  - linear regression
//  - support vector machines
//  - neural networks
//
// The library allows the user to define a certain function using the available tensor operations. This function
// definition is represented internally via a computation graph. Each tensor operation in the function definition
// corresponds to a node in the graph. Having the computation graph defined, the user can choose to compute the
// function's value and/or its gradient with respect to the input variables. Optionally, the function can be optimized
// using one of the available optimization algorithms.
//
// For example, here we define the function: f(x) = a*x^2 + b
//
//   {
//       struct ggml_init_params params = {
//           .mem_size   = 16*1024*1024,
//           .mem_buffer = NULL,
//       };
//
//       // memory allocation happens here
//       struct ggml_context * ctx = ggml_init(params);
//
//       struct ggml_tensor * x = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//
//       ggml_set_param(ctx, x); // x is an input variable
//
//       struct ggml_tensor * a  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//       struct ggml_tensor * b  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//       struct ggml_tensor * x2 = ggml_mul(ctx, x, x);
//       struct ggml_tensor * f  = ggml_add(ctx, ggml_mul(ctx, a, x2), b);
//
//       ...
//   }
//
// Notice that the function definition above does not involve any actual computation. The computation is performed only
// when the user explicitly requests it. For example, to compute the function's value at x = 2.0:
//
//   {
//       ...
//
//       struct ggml_cgraph * gf = ggml_new_graph(ctx);
//       ggml_build_forward_expand(gf, f);
//
//       // set the input variable and parameter values
//       ggml_set_f32(x, 2.0f);
//       ggml_set_f32(a, 3.0f);
//       ggml_set_f32(b, 4.0f);
//
//       ggml_graph_compute_with_ctx(ctx, &gf, n_threads);
//
//       printf("f = %f\n", ggml_get_f32_1d(f, 0));
//
//       ...
//   }
//
// The actual computation is performed in the ggml_graph_compute() function.
//
// The ggml_new_tensor_...() functions create new tensors. They are allocated in the memory buffer provided to the
// ggml_init() function. You have to be careful not to exceed the memory buffer size. Therefore, you have to know
// in advance how much memory you need for your computation. Alternatively, you can allocate a large enough memory
// and after defining the computation graph, call the ggml_used_mem() function to find out how much memory was
// actually needed.
//
// The ggml_set_param() function marks a tensor as an input variable. This is used by the automatic
// differentiation and optimization algorithms.
//
// The described approach allows to define the function graph once and then compute its forward or backward graphs
// multiple times. All computations will use the same memory buffer allocated in the ggml_init() function. This way
// the user can avoid the memory allocation overhead at runtime.
//
// The library supports multi-dimensional tensors - up to 4 dimensions. The FP16 and FP32 data types are first class
// citizens, but in theory the library can be extended to support FP8 and integer data types.
//
// Each tensor operation produces a new tensor. Initially the library was envisioned to support only the use of unary
// and binary operations. Most of the available operations fall into one of these two categories. With time, it became
// clear that the library needs to support more complex operations. The way to support these operations is not clear
// yet, but a few examples are demonstrated in the following operations:
//
//   - ggml_permute()
//   - ggml_conv_1d_1s()
//   - ggml_conv_1d_2s()
//
// For each tensor operator, the library implements a forward and backward computation function. The forward function
// computes the output tensor value given the input tensor values. The backward function computes the adjoint of the
// input tensors given the adjoint of the output tensor. For a detailed explanation of what this means, take a
// calculus class, or watch the following video:
//
//   What is Automatic Differentiation?
//   https://www.youtube.com/watch?v=wG_nF1awSSY
//
//
// ## Tensor data (struct ggml_tensor)
//
// The tensors are stored in memory via the ggml_tensor struct. The structure provides information about the size of
// the tensor, the data type, and the memory buffer where the tensor data is stored. Additionally, it contains
// pointers to the "source" tensors - i.e. the tensors that were used to compute the current tensor. For example:
//
//   {
//       struct ggml_tensor * c = ggml_add(ctx, a, b);
//
//       assert(c->src[0] == a);
//       assert(c->src[1] == b);
//   }
//
// The multi-dimensional tensors are stored in row-major order. The ggml_tensor struct contains fields for the
// number of elements in each dimension ("ne") as well as the number of bytes ("nb", a.k.a. stride). This allows
// to store tensors that are not contiguous in memory, which is useful for operations such as transposition and
// permutation. All tensor operations have to take the stride into account and not assume that the tensor is
// contiguous in memory.
//
// The data of the tensor is accessed via the "data" pointer. For example:
//
//   {
//       const int nx = 2;
//       const int ny = 3;
//
//       struct ggml_tensor * a = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, nx, ny);
//
//       for (int y = 0; y < ny; y++) {
//           for (int x = 0; x < nx; x++) {
//               *(float *) ((char *) a->data + y*a->nb[1] + x*a->nb[0]) = x + y;
//           }
//       }
//
//       ...
//   }
//
// Alternatively, there are helper functions, such as ggml_get_f32_1d() and ggml_set_f32_1d() that can be used.
//
// ## The matrix multiplication operator (ggml_mul_mat)
//
// TODO
//
//
// ## Multi-threading
//
// TODO
//
//
// ## Overview of ggml.c
//
// TODO
//
//
// ## SIMD optimizations
//
// TODO
//
//
// ## Debugging ggml
//
// TODO
//
//


// TODO: #ifdef porting-> llamacpp.h GGML_API



// TODO: #ifdef porting-> llamacpp.h GGML_CALL


// TODO: support for clang

// TODO: #ifdef porting-> llamacpp.h GGML_DEPRECATED(func, hint) func





/* Copyright (C) 2001-2003 by Digital Mars */
/* www.digitalmars.com */





// llamacpp.h __EXPAND(x) x

// llamacpp.h __LONGLONG (__INTSIZE == 4)

/* Exact sizes */
// XXX:TODO see if we really want these mappings or use the regular types
// since the regular types are all defined
// typedef signed char int8_t;
alias int8_t = Typedef!char;
// typedef unsigned char uint8_t;
alias uint8_t = Typedef!ubyte;
// typedef short int16_t;
alias int16_t = Typedef!short;
// typedef unsigned short uint16_t;
alias uint16_t = Typedef!ushort;
// typedef long int32_t;
alias int32_t = Typedef!long;
// typedef unsigned long uint32_t;
alias uint32_t = Typedef!ulong;

/* At least sizes */

// typedef signed char int_least8_t;
alias int_least8_t = Typedef!ubyte;
// typedef unsigned char uint_least8_t;
alias uint_least8_t = Typedef!ubyte;
// typedef short int_least16_t;
alias int_least16_t = Typdef!short;
// typedef unsigned short uint_least16_t;
alias uint_least16_t = Typdef!ushort;
// typedef long int_least32_t;
alias int_least32_t = Typedef!long;
// typedef unsigned long uint_least32_t;
alias uint_least32_t = Typdef!ulong;

/* Fastest minimum width sizes */

// typedef signed char int_fast8_t;
alias int_fast8_t = Typedef!char;
// typedef unsigned char uint_fast8_t;
alias uint_fast8_t = Typedef!ubyte;
// typedef int int_fast16_t;
alias int_fast16_t = Typedef!int;
// typedef unsigned uint_fast16_t;
alias uint_fast16_t = Typedef!unsigned;
// typedef long int_fast32_t;
alias int_fast32_t = Typedef!long;
// typedef unsigned long uint_fast32_t;
alias uint_fast32_t = Typedef!ulong;

/* Integer pointer holders */


// typedef int intptr_t;
alias intptr_t = Typedef!unsigned;
// typedef unsigned uintptr_t;
alias uintptr_t = Typedef!unsigned;

/* Greatest width integer types */


// typedef long long intmax_t;
alias intmax_t = Typedef!long;
//typedef unsigned long long uintmax_t;
alias uintmax_t = Typedef!long;


/* long long typedefs */



// typedef long long int64_t;
alias int64_t = Typedef!long;
// typedef unsigned long long uint64_t;
alias uint64_t = Typedef!ulong;
// typedef long long int_least64_t;
alias int_least64_t = Typedef!long;
// typedef unsigned long long uint_least64_t;
alias uint_least64_t = Typedef!ulong;
// typedef long long int_fast64_t;
alias int_fast64_t = Typedef!long;
// typedef unsigned long long uint_fast64_t;
alias uint_fast64_t = Typedef!ulong;



// llamacpp.h INT8_MIN	(-128)
enum int INT8_MIN = -128;
// llamacpp.h INT8_MAX	127
enum uint INT8_MAX =	127;
// llamacpp.h UINT8_MAX	255
enum uint UINT8_MAX =	255;

// llamacpp.h INT16_MIN	(-32768)
enum int  INT16_MIN =	(-32768);
// llamacpp.h INT16_MAX	32767 
enum int INT16_MAX =	32767;
// llamacpp.h UINT16_MAX	65535
enum int UINT16_MAX =	65535;
// llamacpp.h INT32_MIN	(-2147483647L - 1)
// llamacpp.h INT32_MAX	2147483647
// llamacpp.h UINT32_MAX	4294967295

// llamacpp.h INT_LEAST8_MIN	(-128)
// llamacpp.h INT_LEAST8_MAX	127
// llamacpp.h UINT_LEAST8_MAX	255

// llamacpp.h INT_LEAST16_MIN	(-32768)
// llamacpp.h INT_LEAST16_MAX	32767
// llamacpp.h UINT_LEAST16_MAX 65535

// llamacpp.h INT_LEAST32_MIN	(-2147483647L - 1)
// llamacpp.h INT_LEAST32_MAX	2147483647
// llamacpp.h UINT_LEAST32_MAX 4294967295

// llamacpp.h INT_FAST8_MIN	(-128)
// llamacpp.h INT_FAST8_MAX	127
// llamacpp.h UINT_FAST8_MAX	255


// llamacpp.h INT_FAST16_MIN	(-2147483647L - 1)
// llamacpp.h INT_FAST16_MAX	2147483647
// llamacpp.h UINT_FAST16_MAX	4294967295





// llamacpp.h INT64_MIN	(-9223372036854775807LL-1LL)
// llamacpp.h INT64_MAX	9223372036854775807
// llamacpp.h UINT64_MAX	18446744073709551615

// llamacpp.h INT_LEAST64_MIN	(-9223372036854775807LL-1LL)
// llamacpp.h INT_LEAST64_MAX	9223372036854775807
// llamacpp.h UINT_LEAST64_MAX 18446744073709551615

// llamacpp.h INT_FAST64_MIN	(-9223372036854775807LL-1LL)
// llamacpp.h INT_FAST64_MAX	9223372036854775807
// llamacpp.h UINT_FAST64_MAX	18446744073709551615



// llamacpp.h INTPTR_MIN	(-2147483647L - 1)
// llamacpp.h INTPTR_MAX	2147483647
// llamacpp.h UINTPTR_MAX	4294967295




// llamacpp.h INTMAX_MIN	(-9223372036854775807LL-1LL)
// llamacpp.h INTMAX_MAX	9223372036854775807
// llamacpp.h UINTMAX_MAX	18446744073709551615
// llamacpp.h INTMAX_C(v)	__EXPAND(__EXPAND(v)##LL)
// llamacpp.h UINTMAX_C(v)	__EXPAND(__EXPAND(v)##ULL)




// llamacpp.h PTRDIFF_MIN	(-2147483647L - 1)
// llamacpp.h PTRDIFF_MAX	2147483647
// llamacpp.h SIG_ATOMIC_MIN	(-2147483647L - 1)
// llamacpp.h SIG_ATOMIC_MAX	2147483647
// llamacpp.h SIZE_MAX	4294967295




// llamacpp.h WCHAR_MIN	0
// llamacpp.h WCHAR_MAX	0xFFFF

// llamacpp.h WINT_MIN	0
// llamacpp.h WINT_MAX	65535

// llamacpp.h INT8_C(v)	v
// llamacpp.h UINT8_C(v)	v
// llamacpp.h INT16_C(v)	v
// llamacpp.h UINT16_C(v)	v
// llamacpp.h INT32_C(v)	__EXPAND(__EXPAND(v)##L)
// llamacpp.h UINT32_C(v)	__EXPAND(__EXPAND(v)##UL)


// llamacpp.h INT64_C(v)	__EXPAND(__EXPAND(v)##LL)
// llamacpp.h UINT64_C(v)	__EXPAND(__EXPAND(v)##ULL)



/* Copyright (C) 1986-2001 by Digital Mars. $Revision: 1.1.1.1 $ */





/* Define _CRTAPI1 (for compatibility with the NT SDK) */

// llamacpp.h _CRTAPI1 __cdecl


/* Define _CRTAPI2 (for compatibility with the NT SDK) */

// llamacpp.h _CRTAPI2 __cdecl


/* Define CRTIMP */

// llamacpp.h _CRTIMP



// llamacpp.h __CLIB	__cdecl



// extern int __cdecl errno;



// typedef int ptrdiff_t;
alias ptrdiff_t = Typedef!int;
// typedef unsigned size_t;
alias size_t = Typedef!unsigned;



// llamacpp.h offsetof(t,i)	((size_t)((char *)&((t *)0)->i - (char *)0))








/* Copyright (C) 2001 by Digital Mars */
/* www.digitalmars.com */



// llamacpp.h GGML_FILE_MAGIC   0x67676d6c // "ggml"
// llamacpp.h GGML_FILE_VERSION 1

// llamacpp.h GGML_QNT_VERSION        2    // bump this on quantization format changes
// llamacpp.h GGML_QNT_VERSION_FACTOR 1000 // do not change this

// llamacpp.h GGML_MAX_DIMS           4
// llamacpp.h GGML_MAX_PARAMS         2048
// llamacpp.h GGML_MAX_CONTEXTS       64
// llamacpp.h GGML_MAX_SRC            10

// llamacpp.h GGML_MAX_NAME           64

// llamacpp.h GGML_MAX_OP_PARAMS      64
// llamacpp.h GGML_DEFAULT_N_THREADS  4
// llamacpp.h GGML_DEFAULT_GRAPH_SIZE 2048

    // llamacpp.h GGML_MEM_ALIGN 4



// llamacpp.h GGML_EXIT_SUCCESS 0
// llamacpp.h GGML_EXIT_ABORTED 1

// llamacpp.h GGUF_MAGIC "GGUF"

// llamacpp.h GGUF_VERSION 3

// llamacpp.h GGUF_DEFAULT_ALIGNMENT 32

// llamacpp.h GGML_UNUSED(x) (void)(x)

// llamacpp.h GGML_PAD(x, n) (((x) + (n) - 1) & ~((n) - 1))

// llamacpp.h GGML_ASSERT(x)     do {         if (!(x)) {             fflush(stdout);             fprintf(stderr, "GGML_ASSERT: %s:%d: %s\n", __FILE__, __LINE__, #x);             ggml_print_backtrace();             abort();         }     } while (0)




// used to copy the number of elements and stride in bytes of tensors into local variables.
// main purpose is to reduce code duplication and improve readability.
//
// example:
//
//    GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne);
//    GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb);
//
// llamacpp.h GGML_TENSOR_LOCALS_1(type, prefix, pointer, array)     const type prefix##0 = (pointer)->array[0];     GGML_UNUSED(prefix##0);
// llamacpp.h GGML_TENSOR_LOCALS_2(type, prefix, pointer, array)     GGML_TENSOR_LOCALS_1    (type, prefix, pointer, array)     const type prefix##1 = (pointer)->array[1];     GGML_UNUSED(prefix##1);
// llamacpp.h GGML_TENSOR_LOCALS_3(type, prefix, pointer, array)     GGML_TENSOR_LOCALS_2    (type, prefix, pointer, array)     const type prefix##2 = (pointer)->array[2];     GGML_UNUSED(prefix##2);
// llamacpp.h GGML_TENSOR_LOCALS(type, prefix, pointer, array)     GGML_TENSOR_LOCALS_3  (type, prefix, pointer, array)     const type prefix##3 = (pointer)->array[3];     GGML_UNUSED(prefix##3);

// llamacpp.h GGML_TENSOR_UNARY_OP_LOCALS     GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne)     GGML_TENSOR_LOCALS(size_t,  nb0, src0, nb)     GGML_TENSOR_LOCALS(int64_t, ne,  dst,  ne)     GGML_TENSOR_LOCALS(size_t,  nb,  dst,  nb)

// llamacpp.h GGML_TENSOR_BINARY_OP_LOCALS     GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne)     GGML_TENSOR_LOCALS(size_t,  nb0, src0, nb)     GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne)     GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb)     GGML_TENSOR_LOCALS(int64_t, ne,  dst,  ne)     GGML_TENSOR_LOCALS(size_t,  nb,  dst,  nb)






    // convert FP16 <-> FP32
     float       ggml_fp16_to_fp32(ggml_fp16_t x);
     ggml_fp16_t ggml_fp32_to_fp16(float x);

     void ggml_fp16_to_fp32_row(const ggml_fp16_t * x, float * y, int n);
     void ggml_fp32_to_fp16_row(const float * x, ggml_fp16_t * y, int n);

    struct ggml_object;
    struct ggml_context;

    enum ggml_type {
        GGML_TYPE_F32  = 0,
        GGML_TYPE_F16  = 1,
        GGML_TYPE_Q4_0 = 2,
        GGML_TYPE_Q4_1 = 3,
        // GGML_TYPE_Q4_2 = 4, support has been removed
        // GGML_TYPE_Q4_3 (5) support has been removed
        GGML_TYPE_Q5_0 = 6,
        GGML_TYPE_Q5_1 = 7,
        GGML_TYPE_Q8_0 = 8,
        GGML_TYPE_Q8_1 = 9,
        // k-quantizations
        GGML_TYPE_Q2_K = 10,
        GGML_TYPE_Q3_K = 11,
        GGML_TYPE_Q4_K = 12,
        GGML_TYPE_Q5_K = 13,
        GGML_TYPE_Q6_K = 14,
        GGML_TYPE_Q8_K = 15,
        GGML_TYPE_IQ2_XXS = 16,
        GGML_TYPE_IQ2_XS  = 17,
        GGML_TYPE_IQ3_XXS = 18,
        GGML_TYPE_I8,
        GGML_TYPE_I16,
        GGML_TYPE_I32,
        GGML_TYPE_COUNT,
    };

    // precision
    enum ggml_prec {
        GGML_PREC_DEFAULT,
        GGML_PREC_F32,
    };

    enum ggml_backend_type {
        GGML_BACKEND_CPU = 0,
        GGML_BACKEND_GPU = 10,
        GGML_BACKEND_GPU_SPLIT = 20,
    };

    // model file types
    enum ggml_ftype {
        GGML_FTYPE_UNKNOWN     = -1,
        GGML_FTYPE_ALL_F32     = 0,
        GGML_FTYPE_MOSTLY_F16  = 1,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_0 = 2,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_1 = 3,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_1_SOME_F16 = 4, // tok_embeddings.weight and output.weight are F16
        GGML_FTYPE_MOSTLY_Q8_0 = 7,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_0 = 8,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_1 = 9,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q2_K = 10, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q3_K = 11, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_K = 12, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_K = 13, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q6_K = 14, // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ2_XXS = 15, // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ2_XS  = 16, // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ3_XXS = 17, // except 1d tensors
    };

    // available tensor operations:
    enum ggml_op {
        GGML_OP_NONE = 0,

        GGML_OP_DUP,
        GGML_OP_ADD,
        GGML_OP_ADD1,
        GGML_OP_ACC,
        GGML_OP_SUB,
        GGML_OP_MUL,
        GGML_OP_DIV,
        GGML_OP_SQR,
        GGML_OP_SQRT,
        GGML_OP_LOG,
        GGML_OP_SUM,
        GGML_OP_SUM_ROWS,
        GGML_OP_MEAN,
        GGML_OP_ARGMAX,
        GGML_OP_REPEAT,
        GGML_OP_REPEAT_BACK,
        GGML_OP_CONCAT,
        GGML_OP_SILU_BACK,
        GGML_OP_NORM, // normalize
        GGML_OP_RMS_NORM,
        GGML_OP_RMS_NORM_BACK,
        GGML_OP_GROUP_NORM,

        GGML_OP_MUL_MAT,
        GGML_OP_MUL_MAT_ID,
        GGML_OP_OUT_PROD,

        GGML_OP_SCALE,
        GGML_OP_SET,
        GGML_OP_CPY,
        GGML_OP_CONT,
        GGML_OP_RESHAPE,
        GGML_OP_VIEW,
        GGML_OP_PERMUTE,
        GGML_OP_TRANSPOSE,
        GGML_OP_GET_ROWS,
        GGML_OP_GET_ROWS_BACK,
        GGML_OP_DIAG,
        GGML_OP_DIAG_MASK_INF,
        GGML_OP_DIAG_MASK_ZERO,
        GGML_OP_SOFT_MAX,
        GGML_OP_SOFT_MAX_BACK,
        GGML_OP_ROPE,
        GGML_OP_ROPE_BACK,
        GGML_OP_ALIBI,
        GGML_OP_CLAMP,
        GGML_OP_CONV_TRANSPOSE_1D,
        GGML_OP_IM2COL,
        GGML_OP_CONV_TRANSPOSE_2D,
        GGML_OP_POOL_1D,
        GGML_OP_POOL_2D,
        GGML_OP_UPSCALE, // nearest interpolate
        GGML_OP_PAD,
        GGML_OP_ARGSORT,
        GGML_OP_LEAKY_RELU,

        GGML_OP_FLASH_ATTN,
        GGML_OP_FLASH_FF,
        GGML_OP_FLASH_ATTN_BACK,
        GGML_OP_WIN_PART,
        GGML_OP_WIN_UNPART,
        GGML_OP_GET_REL_POS,
        GGML_OP_ADD_REL_POS,

        GGML_OP_UNARY,

        GGML_OP_MAP_UNARY,
        GGML_OP_MAP_BINARY,

        GGML_OP_MAP_CUSTOM1_F32,
        GGML_OP_MAP_CUSTOM2_F32,
        GGML_OP_MAP_CUSTOM3_F32,

        GGML_OP_MAP_CUSTOM1,
        GGML_OP_MAP_CUSTOM2,
        GGML_OP_MAP_CUSTOM3,

        GGML_OP_CROSS_ENTROPY_LOSS,
        GGML_OP_CROSS_ENTROPY_LOSS_BACK,

        GGML_OP_COUNT,
    };

    enum ggml_unary_op {
        GGML_UNARY_OP_ABS,
        GGML_UNARY_OP_SGN,
        GGML_UNARY_OP_NEG,
        GGML_UNARY_OP_STEP,
        GGML_UNARY_OP_TANH,
        GGML_UNARY_OP_ELU,
        GGML_UNARY_OP_RELU,
        GGML_UNARY_OP_GELU,
        GGML_UNARY_OP_GELU_QUICK,
        GGML_UNARY_OP_SILU,
        GGML_UNARY_OP_HARDSWISH,
        GGML_UNARY_OP_HARDSIGMOID,

        GGML_UNARY_OP_COUNT,
    };

    enum ggml_object_type {
        GGML_OBJECT_TENSOR,
        GGML_OBJECT_GRAPH,
        GGML_OBJECT_WORK_BUFFER
    };

    enum ggml_log_level {
        GGML_LOG_LEVEL_ERROR = 2,
        GGML_LOG_LEVEL_WARN  = 3,
        GGML_LOG_LEVEL_INFO  = 4,
        GGML_LOG_LEVEL_DEBUG = 5
    };

    enum ggml_tensor_flag {
        GGML_TENSOR_FLAG_INPUT  = 1,
        GGML_TENSOR_FLAG_OUTPUT = 2,
        GGML_TENSOR_FLAG_PARAM  = 4,
    };

//
// XXX CONTINUE PORTING BELOW
// TOGGLE LINE COMMENT
//  VVVVVVVVVVVVVVVVVVVVVV

//     // ggml object
//     struct ggml_object {
//         size_t offs;
//         size_t size;

//         struct ggml_object * next;

//         enum ggml_object_type type;

//         char padding[4];
//     };

//     static const size_t GGML_OBJECT_SIZE = sizeof(struct ggml_object);

//     // n-dimensional tensor
//     struct ggml_tensor {
//         enum ggml_type         type;
//         enum ggml_backend_type backend;

//         struct ggml_backend_buffer * buffer;

//         int64_t ne[4]; // number of elements
//         size_t  nb[4]; // stride in bytes:
//                                    // nb[0] = ggml_type_size(type)
//                                    // nb[1] = nb[0]   * (ne[0] / ggml_blck_size(type)) + padding
//                                    // nb[i] = nb[i-1] * ne[i-1]

//         // compute data
//         enum ggml_op op;

//         // op params - allocated as int32_t for alignment
//         int32_t op_params[64 / sizeof(int32_t)];

//         int32_t flags;

//         struct ggml_tensor * grad;
//         struct ggml_tensor * src[10];

//         // performance
//         int     perf_runs;
//         int64_t perf_cycles;
//         int64_t perf_time_us;

//         struct ggml_tensor * view_src;
//         size_t               view_offs;

//         void * data;

//         char name[64];

//         void * extra; // extra things e.g. for ggml-cuda.cu

//         char padding[8];
//     };

//     static const size_t GGML_TENSOR_SIZE = sizeof(struct ggml_tensor);

//     // Abort callback
//     // If not NULL, called before ggml computation
//     // If it returns true, the computation is aborted
//     typedef _Bool (*ggml_abort_callback)(void * data);

//     // the compute plan that needs to be prepared for ggml_graph_compute()
//     // since https://github.com/ggerganov/ggml/issues/287
//     struct ggml_cplan {
//         size_t    work_size; // size of work buffer, calculated by `ggml_graph_plan()`
//         uint8_t * work_data; // work buffer, to be allocated by caller before calling to `ggml_graph_compute()`

//         int n_threads;

//         // abort ggml_graph_compute when true
//         ggml_abort_callback abort_callback;
//         void *              abort_callback_data;
//     };

//     enum ggml_cgraph_eval_order {
//         GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT = 0,
//         GGML_CGRAPH_EVAL_ORDER_RIGHT_TO_LEFT,
//         GGML_CGRAPH_EVAL_ORDER_COUNT
//     };

//     struct ggml_hash_set {
//         size_t size;
//         struct ggml_tensor ** keys;
//     };

//     // computation graph
//     struct ggml_cgraph {
//         int size;
//         int n_nodes;
//         int n_leafs;

//         struct ggml_tensor ** nodes;
//         struct ggml_tensor ** grads;
//         struct ggml_tensor ** leafs;

//         struct ggml_hash_set visited_hash_table;

//         enum ggml_cgraph_eval_order order;

//         // performance
//         int     perf_runs;
//         int64_t perf_cycles;
//         int64_t perf_time_us;
//     };

//     // scratch buffer
//     struct ggml_scratch {
//         size_t offs;
//         size_t size;
//         void * data;
//     };

//     struct ggml_init_params {
//         // memory pool
//         size_t mem_size;   // bytes
//         void * mem_buffer; // if NULL, memory will be allocated internally
//         _Bool   no_alloc;   // don't allocate memory for the tensor data
//     };


//     // compute types

//     // NOTE: the INIT or FINALIZE pass is not scheduled unless explicitly enabled.
//     // This behavior was changed since https://github.com/ggerganov/llama.cpp/pull/1995.
//     enum ggml_task_type {
//         GGML_TASK_INIT = 0,
//         GGML_TASK_COMPUTE,
//         GGML_TASK_FINALIZE,
//     };

//     struct ggml_compute_params {
//         enum ggml_task_type type;

//         // ith = thread index, nth = number of threads
//         int ith, nth;

//         // work buffer for all threads
//         size_t wsize;
//         void * wdata;
//     };

//     // numa strategies
//     enum ggml_numa_strategy {
//         GGML_NUMA_STRATEGY_DISABLED   = 0,
//         GGML_NUMA_STRATEGY_DISTRIBUTE = 1,
//         GGML_NUMA_STRATEGY_ISOLATE    = 2,
//         GGML_NUMA_STRATEGY_NUMACTL    = 3,
//         GGML_NUMA_STRATEGY_MIRROR     = 4,
//         GGML_NUMA_STRATEGY_COUNT
//     };

//     // misc

//      void    ggml_time_init(void); // call this once at the beginning of the program
//      int64_t ggml_time_ms(void);
//      int64_t ggml_time_us(void);
//      int64_t ggml_cycles(void);
//      int64_t ggml_cycles_per_ms(void);

//      void    ggml_print_backtrace(void);

//      void    ggml_numa_init(enum ggml_numa_strategy numa); // call once for better performance on NUMA systems
//      _Bool    ggml_is_numa(void); // true if init detected that system has >1 NUMA node

//      void    ggml_print_object (const struct ggml_object * obj);
//      void    ggml_print_objects(const struct ggml_context * ctx);

//       int64_t ggml_nelements   (const struct ggml_tensor * tensor);
//       int64_t ggml_nrows       (const struct ggml_tensor * tensor);
//       size_t  ggml_nbytes      (const struct ggml_tensor * tensor);
//                size_t  ggml_nbytes_pad  (const struct ggml_tensor * tensor); // same as ggml_nbytes() but padded to GGML_MEM_ALIGN

//       int    ggml_blck_size(enum ggml_type type);
//       size_t ggml_type_size(enum ggml_type type);             // size in bytes for all elements in a block
//       size_t ggml_row_size (enum ggml_type type, int64_t ne); // size in bytes for all elements in a row

//     double ggml_type_sizef(enum ggml_type type);

//       const char * ggml_type_name(enum ggml_type type);
//       const char * ggml_op_name  (enum ggml_op   op);
//                const char * ggml_op_symbol(enum ggml_op   op);

//                const char * ggml_unary_op_name(enum ggml_unary_op op);
//       const char * ggml_op_desc(const struct ggml_tensor * t); // unary or op name

//       size_t  ggml_element_size(const struct ggml_tensor * tensor);

//       _Bool    ggml_is_quantized(enum ggml_type type);

//     // TODO: temporary until model loading of ggml examples is refactored
//      enum ggml_type ggml_ftype_to_ggml_type(enum ggml_ftype ftype);

//       _Bool ggml_is_transposed(const struct ggml_tensor * tensor);
//       _Bool ggml_is_contiguous(const struct ggml_tensor * tensor);
//       _Bool ggml_is_permuted  (const struct ggml_tensor * tensor);
//                _Bool ggml_is_scalar    (const struct ggml_tensor * tensor);
//                _Bool ggml_is_vector    (const struct ggml_tensor * tensor);
//                _Bool ggml_is_matrix    (const struct ggml_tensor * tensor);
//                _Bool ggml_is_3d        (const struct ggml_tensor * tensor);
//                int  ggml_n_dims       (const struct ggml_tensor * tensor); // returns 1 for scalars

//      _Bool ggml_are_same_shape(const struct ggml_tensor * t0, const struct ggml_tensor * t1);

//     // use this to compute the memory overhead of a tensor
//      size_t ggml_tensor_overhead(void);

//     // main

//      struct ggml_context * ggml_init(struct ggml_init_params params);
//      void                  ggml_free(struct ggml_context * ctx);

//      size_t  ggml_used_mem(const struct ggml_context * ctx);

//      size_t  ggml_set_scratch (struct ggml_context * ctx, struct ggml_scratch scratch);
//      _Bool    ggml_get_no_alloc(struct ggml_context * ctx);
//      void    ggml_set_no_alloc(struct ggml_context * ctx, _Bool no_alloc);

//      void *  ggml_get_mem_buffer     (const struct ggml_context * ctx);
//      size_t  ggml_get_mem_size       (const struct ggml_context * ctx);
//      size_t  ggml_get_max_tensor_size(const struct ggml_context * ctx);

//      struct ggml_tensor * ggml_new_tensor(
//             struct ggml_context * ctx,
//             enum   ggml_type type,
//             int    n_dims,
//             const int64_t *ne);

//      struct ggml_tensor * ggml_new_tensor_1d(
//             struct ggml_context * ctx,
//             enum   ggml_type type,
//             int64_t ne0);

//      struct ggml_tensor * ggml_new_tensor_2d(
//             struct ggml_context * ctx,
//             enum   ggml_type type,
//             int64_t ne0,
//             int64_t ne1);

//      struct ggml_tensor * ggml_new_tensor_3d(
//             struct ggml_context * ctx,
//             enum   ggml_type type,
//             int64_t ne0,
//             int64_t ne1,
//             int64_t ne2);

//      struct ggml_tensor * ggml_new_tensor_4d(
//             struct ggml_context * ctx,
//             enum   ggml_type type,
//             int64_t ne0,
//             int64_t ne1,
//             int64_t ne2,
//             int64_t ne3);

//      struct ggml_tensor * ggml_new_i32(struct ggml_context * ctx, int32_t value);
//      struct ggml_tensor * ggml_new_f32(struct ggml_context * ctx, float value);

//      struct ggml_tensor * ggml_dup_tensor (struct ggml_context * ctx, const struct ggml_tensor * src);
//      struct ggml_tensor * ggml_view_tensor(struct ggml_context * ctx, struct ggml_tensor * src);

//     // Context tensor enumeration and lookup
//      struct ggml_tensor * ggml_get_first_tensor(const struct ggml_context * ctx);
//      struct ggml_tensor * ggml_get_next_tensor (const struct ggml_context * ctx, struct ggml_tensor * tensor);
//      struct ggml_tensor * ggml_get_tensor(struct ggml_context * ctx, const char * name);

//      struct ggml_tensor * ggml_set_zero(struct ggml_tensor * tensor);
//      struct ggml_tensor * ggml_set_i32 (struct ggml_tensor * tensor, int32_t value);
//      struct ggml_tensor * ggml_set_f32 (struct ggml_tensor * tensor, float value);

//     // Converts a flat index into coordinates
//      void    ggml_unravel_index(const struct ggml_tensor * tensor, int64_t i, int64_t * i0, int64_t * i1, int64_t * i2, int64_t * i3);

//      int32_t ggml_get_i32_1d(const struct ggml_tensor * tensor, int i);
//      void    ggml_set_i32_1d(const struct ggml_tensor * tensor, int i, int32_t value);

//      int32_t ggml_get_i32_nd(const struct ggml_tensor * tensor, int i0, int i1, int i2, int i3);
//      void    ggml_set_i32_nd(const struct ggml_tensor * tensor, int i0, int i1, int i2, int i3, int32_t value);

//      float   ggml_get_f32_1d(const struct ggml_tensor * tensor, int i);
//      void    ggml_set_f32_1d(const struct ggml_tensor * tensor, int i, float value);

//      float   ggml_get_f32_nd(const struct ggml_tensor * tensor, int i0, int i1, int i2, int i3);
//      void    ggml_set_f32_nd(const struct ggml_tensor * tensor, int i0, int i1, int i2, int i3, float value);

//      void *  ggml_get_data    (const struct ggml_tensor * tensor);
//      float * ggml_get_data_f32(const struct ggml_tensor * tensor);

//       enum ggml_unary_op ggml_get_unary_op(const struct ggml_tensor * tensor);

//      const char *         ggml_get_name   (const struct ggml_tensor * tensor);
//      struct ggml_tensor * ggml_set_name   (      struct ggml_tensor * tensor, const char * name);
    
//      struct ggml_tensor * ggml_format_name(      struct ggml_tensor * tensor, const char * fmt, ...);

//     //
//     // operations on tensors with backpropagation
//     //

//      struct ggml_tensor * ggml_dup(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//     // in-place, returns view(a)
//      struct ggml_tensor * ggml_dup_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_add(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//      struct ggml_tensor * ggml_add_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//      struct ggml_tensor * ggml_add_cast(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             enum   ggml_type      type);

//      struct ggml_tensor * ggml_add1(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//      struct ggml_tensor * ggml_add1_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//     // dst = a
//     // view(dst, nb1, nb2, nb3, offset) += b
//     // return dst
//      struct ggml_tensor * ggml_acc(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             size_t                nb1,
//             size_t                nb2,
//             size_t                nb3,
//             size_t                offset);

//      struct ggml_tensor * ggml_acc_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             size_t                nb1,
//             size_t                nb2,
//             size_t                nb3,
//             size_t                offset);

//      struct ggml_tensor * ggml_sub(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//      struct ggml_tensor * ggml_sub_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//      struct ggml_tensor * ggml_mul(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//      struct ggml_tensor * ggml_mul_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//      struct ggml_tensor * ggml_div(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//      struct ggml_tensor * ggml_div_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//      struct ggml_tensor * ggml_sqr(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_sqr_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_sqrt(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_sqrt_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_log(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_log_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//     // return scalar
//      struct ggml_tensor * ggml_sum(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//     // sums along rows, with input shape [a,b,c,d] return shape [1,b,c,d]
//      struct ggml_tensor * ggml_sum_rows(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//     // mean along rows
//      struct ggml_tensor * ggml_mean(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//     // argmax along rows
//      struct ggml_tensor * ggml_argmax(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//     // if a is the same shape as b, and a is not parameter, return a
//     // otherwise, return a new tensor: repeat(a) to fit in b
//      struct ggml_tensor * ggml_repeat(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//     // sums repetitions in a into shape of b
//      struct ggml_tensor * ggml_repeat_back(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//     // concat a and b on dim 2
//     // used in stable-diffusion
//      struct ggml_tensor * ggml_concat(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//      struct ggml_tensor * ggml_abs(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_abs_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_sgn(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_sgn_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_neg(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_neg_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_step(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_step_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_tanh(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_tanh_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_elu(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_elu_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_relu(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_leaky_relu(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a, float negative_slope, _Bool inplace);

//      struct ggml_tensor * ggml_relu_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_gelu(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_gelu_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_gelu_quick(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_gelu_quick_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_silu(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//      struct ggml_tensor * ggml_silu_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//     // a - x
//     // b - dy
//      struct ggml_tensor * ggml_silu_back(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//     // hardswish(x) = x * relu6(x + 3) / 6
//      struct ggml_tensor * ggml_hardswish(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//     // hardsigmoid(x) = relu6(x + 3) / 6
//      struct ggml_tensor * ggml_hardsigmoid(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//     // normalize along rows
//      struct ggml_tensor * ggml_norm(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             float                 eps);

//      struct ggml_tensor * ggml_norm_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             float                 eps);

//      struct ggml_tensor * ggml_rms_norm(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             float                 eps);

//      struct ggml_tensor * ggml_rms_norm_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             float                 eps);

//     // group normalize along ne0*ne1*n_groups
//     // used in stable-diffusion
//     // TODO: eps is hardcoded to 1e-6 for now
//      struct ggml_tensor * ggml_group_norm(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int                   n_groups);

//      struct ggml_tensor * ggml_group_norm_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int                   n_groups);

//     // a - x
//     // b - dy
//      struct ggml_tensor * ggml_rms_norm_back(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             float                 eps);

//     // A: k columns, n rows => [ne03, ne02, n, k]
//     // B: k columns, m rows  (i.e. we transpose it internally) => [ne03 * x, ne02 * y, m, k]
//     // result is n columns, m rows => [ne03 * x, ne02 * y, m, n]
//      struct ggml_tensor * ggml_mul_mat(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//     // change the precision of a matrix multiplication
//     // set to GGML_PREC_F32 for higher precision (useful for phi-2)
//      void ggml_mul_mat_set_prec(
//             struct ggml_tensor * a,
//             enum ggml_prec       prec);

//     // indirect matrix multiplication
//     //  ggml_mul_mat_id(ctx, as, ids, id, b) ~= ggml_mul_mat(as[ids[id]], b)
//      struct ggml_tensor * ggml_mul_mat_id(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * const as[],
//             int                   n_as,
//             struct ggml_tensor  * ids,
//             int                   id,
//             struct ggml_tensor  * b);

//     // A: m columns, n rows,
//     // B: p columns, n rows,
//     // result is m columns, p rows
//      struct ggml_tensor * ggml_out_prod(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//     //
//     // operations on tensors without backpropagation
//     //

//      struct ggml_tensor * ggml_scale(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             float                 s);

//     // in-place, returns view(a)
//      struct ggml_tensor * ggml_scale_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             float                 s);

//     // b -> view(a,offset,nb1,nb2,3), return modified a
//      struct ggml_tensor * ggml_set(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             size_t                nb1,
//             size_t                nb2,
//             size_t                nb3,
//             size_t                offset);

//     // b -> view(a,offset,nb1,nb2,3), return view(a)
//      struct ggml_tensor * ggml_set_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             size_t                nb1,
//             size_t                nb2,
//             size_t                nb3,
//             size_t                offset);

//      struct ggml_tensor * ggml_set_1d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             size_t                offset);

//      struct ggml_tensor * ggml_set_1d_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             size_t                offset);

//     // b -> view(a,offset,nb1,nb2,3), return modified a
//      struct ggml_tensor * ggml_set_2d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             size_t                nb1,
//             size_t                offset);

//     // b -> view(a,offset,nb1,nb2,3), return view(a)
//      struct ggml_tensor * ggml_set_2d_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             size_t                nb1,
//             size_t                offset);

//     // a -> b, return view(b)
//      struct ggml_tensor * ggml_cpy(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//      struct ggml_tensor * ggml_cast(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             enum   ggml_type      type);

//     // make contiguous
//      struct ggml_tensor * ggml_cont(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//     // make contiguous, with new shape
//      struct ggml_tensor * ggml_cont_1d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int64_t               ne0);

//      struct ggml_tensor * ggml_cont_2d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int64_t               ne0,
//             int64_t               ne1);

//      struct ggml_tensor * ggml_cont_3d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int64_t               ne0,
//             int64_t               ne1,
//             int64_t               ne2);

//      struct ggml_tensor * ggml_cont_4d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int64_t               ne0,
//             int64_t               ne1,
//             int64_t               ne2,
//             int64_t               ne3);

//     // return view(a), b specifies the new shape
//     // TODO: when we start computing gradient, make a copy instead of view
//      struct ggml_tensor * ggml_reshape(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//     // return view(a)
//     // TODO: when we start computing gradient, make a copy instead of view
//      struct ggml_tensor * ggml_reshape_1d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int64_t               ne0);

//      struct ggml_tensor * ggml_reshape_2d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int64_t               ne0,
//             int64_t               ne1);

//     // return view(a)
//     // TODO: when we start computing gradient, make a copy instead of view
//      struct ggml_tensor * ggml_reshape_3d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int64_t               ne0,
//             int64_t               ne1,
//             int64_t               ne2);

//      struct ggml_tensor * ggml_reshape_4d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int64_t               ne0,
//             int64_t               ne1,
//             int64_t               ne2,
//             int64_t               ne3);

//     // offset in bytes
//      struct ggml_tensor * ggml_view_1d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int64_t               ne0,
//             size_t                offset);

//      struct ggml_tensor * ggml_view_2d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int64_t               ne0,
//             int64_t               ne1,
//             size_t                nb1, // row stride in bytes
//             size_t                offset);

//      struct ggml_tensor * ggml_view_3d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int64_t               ne0,
//             int64_t               ne1,
//             int64_t               ne2,
//             size_t                nb1, // row   stride in bytes
//             size_t                nb2, // slice stride in bytes
//             size_t                offset);

//      struct ggml_tensor * ggml_view_4d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int64_t               ne0,
//             int64_t               ne1,
//             int64_t               ne2,
//             int64_t               ne3,
//             size_t                nb1, // row   stride in bytes
//             size_t                nb2, // slice stride in bytes
//             size_t                nb3,
//             size_t                offset);

//      struct ggml_tensor * ggml_permute(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int                   axis0,
//             int                   axis1,
//             int                   axis2,
//             int                   axis3);

//     // alias for ggml_permute(ctx, a, 1, 0, 2, 3)
//      struct ggml_tensor * ggml_transpose(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//     // supports 3D: a->ne[2] == b->ne[1]
//      struct ggml_tensor * ggml_get_rows(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//      struct ggml_tensor * ggml_get_rows_back(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             struct ggml_tensor  * c);

//      struct ggml_tensor * ggml_diag(
//         struct ggml_context     * ctx,
//         struct ggml_tensor      * a);

//     // set elements above the diagonal to -INF
//      struct ggml_tensor * ggml_diag_mask_inf(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int                   n_past);

//     // in-place, returns view(a)
//      struct ggml_tensor * ggml_diag_mask_inf_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int                   n_past);

//     // set elements above the diagonal to 0
//      struct ggml_tensor * ggml_diag_mask_zero(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int                   n_past);

//     // in-place, returns view(a)
//      struct ggml_tensor * ggml_diag_mask_zero_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int                   n_past);

//      struct ggml_tensor * ggml_soft_max(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//     // in-place, returns view(a)
//      struct ggml_tensor * ggml_soft_max_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a);

//     // fused soft_max(a*scale + mask)
//     // mask is optional
//      struct ggml_tensor * ggml_soft_max_ext(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * mask,
//             float                 scale);

//      struct ggml_tensor * ggml_soft_max_back(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//     // in-place, returns view(a)
//      struct ggml_tensor * ggml_soft_max_back_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//     // rotary position embedding
//     // if mode & 1 == 1, skip n_past elements (DEPRECATED)
//     // if mode & 2 == 1, GPT-NeoX style
//     // if mode & 4 == 1, ChatGLM style
//     //
//     // b is an int32 vector with size a->ne[2], it contains the positions
//      struct ggml_tensor * ggml_rope(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             int                   n_dims,
//             int                   mode,
//             int                   n_ctx);

//     // in-place, returns view(a)
//      struct ggml_tensor * ggml_rope_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             int                   n_dims,
//             int                   mode,
//             int                   n_ctx);

//     // custom RoPE
//      struct ggml_tensor * ggml_rope_custom(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             int                   n_dims,
//             int                   mode,
//             int                   n_ctx,
//             int                   n_orig_ctx,
//             float                 freq_base,
//             float                 freq_scale,
//             float                 ext_factor,
//             float                 attn_factor,
//             float                 beta_fast,
//             float                 beta_slow);

//     // in-place, returns view(a)
//      struct ggml_tensor * ggml_rope_custom_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             int                   n_dims,
//             int                   mode,
//             int                   n_ctx,
//             int                   n_orig_ctx,
//             float                 freq_base,
//             float                 freq_scale,
//             float                 ext_factor,
//             float                 attn_factor,
//             float                 beta_fast,
//             float                 beta_slow);

//     // compute correction dims for YaRN RoPE scaling
//      void ggml_rope_yarn_corr_dims(
//         int n_dims, int n_orig_ctx, float freq_base, float beta_fast, float beta_slow, float dims[2]);

//     // xPos RoPE, in-place, returns view(a)
//      struct ggml_tensor * ggml_rope_xpos_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             int                   n_dims,
//             float                 base,
//             _Bool                  down);

//     // rotary position embedding backward, i.e compute dx from dy
//     // a - dy
//      struct ggml_tensor * ggml_rope_back(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             int                   n_dims,
//             int                   mode,
//             int                   n_ctx,
//             int                   n_orig_ctx,
//             float                 freq_base,
//             float                 freq_scale,
//             float                 ext_factor,
//             float                 attn_factor,
//             float                 beta_fast,
//             float                 beta_slow,
//             float                 xpos_base,
//             _Bool                  xpos_down);

//     // alibi position embedding
//     // in-place, returns view(a)
//      struct ggml_tensor * ggml_alibi(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int                   n_past,
//             int                   n_head,
//             float                 bias_max);

//     // clamp
//     // in-place, returns view(a)
//      struct ggml_tensor * ggml_clamp(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             float                 min,
//             float                 max);

//      struct ggml_tensor * ggml_im2col(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             int                  s0,
//             int                  s1,
//             int                  p0,
//             int                  p1,
//             int                  d0,
//             int                  d1,
//             _Bool                 is_2D,
//             enum ggml_type       dst_type);

//      struct ggml_tensor * ggml_conv_depthwise_2d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             int                  s0,
//             int                  s1,
//             int                  p0,
//             int                  p1,
//             int                  d0,
//             int                  d1);

//      struct ggml_tensor * ggml_conv_1d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             int                   s0,  // stride
//             int                   p0,  // padding
//             int                   d0); // dilation

//     // conv_1d with padding = half
//     // alias for ggml_conv_1d(a, b, s, a->ne[0]/2, d)
//      struct ggml_tensor* ggml_conv_1d_ph(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             int                   s,
//             int                   d);

//      struct ggml_tensor * ggml_conv_transpose_1d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             int                   s0,
//             int                   p0,
//             int                   d0);

//      struct ggml_tensor * ggml_conv_2d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             int                   s0,
//             int                   s1,
//             int                   p0,
//             int                   p1,
//             int                   d0,
//             int                   d1);


//     // kernel size is a->ne[0] x a->ne[1]
//     // stride is equal to kernel size
//     // padding is zero
//     // example:
//     // a:     16   16    3  768
//     // b:   1024 1024    3    1
//     // res:   64   64  768    1
//     // used in sam
//      struct ggml_tensor * ggml_conv_2d_sk_p0(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//     // kernel size is a->ne[0] x a->ne[1]
//     // stride is 1
//     // padding is half
//     // example:
//     // a:      3    3    256  256
//     // b:     64   64    256    1
//     // res:   64   64    256    1
//     // used in sam
//      struct ggml_tensor * ggml_conv_2d_s1_ph(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b);

//      struct ggml_tensor * ggml_conv_transpose_2d_p0(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b,
//             int                   stride);

//     enum ggml_op_pool {
//         GGML_OP_POOL_MAX,
//         GGML_OP_POOL_AVG,
//         GGML_OP_POOL_COUNT,
//     };

//      struct ggml_tensor * ggml_pool_1d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             enum ggml_op_pool     op,
//             int                   k0, // kernel size
//             int                   s0, // stride
//             int                   p0); // padding

//     // the result will have 2*p0 padding for the first dimension
//     // and 2*p1 padding for the second dimension
//      struct ggml_tensor * ggml_pool_2d(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             enum ggml_op_pool     op,
//             int                   k0,
//             int                   k1,
//             int                   s0,
//             int                   s1,
//             float                 p0,
//             float                 p1);

//     // nearest interpolate
//     // used in stable-diffusion
//      struct ggml_tensor * ggml_upscale(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int                   scale_factor);

//     // pad each dimension with zeros: [x, ..., x] -> [x, ..., x, 0, ..., 0]
//      struct ggml_tensor * ggml_pad(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int                  p0,
//             int                  p1,
//             int                  p2,
//             int                  p3);

//     // sort rows
//     enum ggml_sort_order {
//         GGML_SORT_ASC,
//         GGML_SORT_DESC,
//     };

//      struct ggml_tensor * ggml_argsort(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             enum ggml_sort_order  order);

//     // top k elements per row
//      struct ggml_tensor * ggml_top_k(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int                   k);

//      struct ggml_tensor * ggml_flash_attn(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * q,
//             struct ggml_tensor  * k,
//             struct ggml_tensor  * v,
//             _Bool                  masked);

//      struct ggml_tensor * ggml_flash_attn_back(
//            struct ggml_context * ctx,
//            struct ggml_tensor  * q,
//            struct ggml_tensor  * k,
//            struct ggml_tensor  * v,
//            struct ggml_tensor  * d,
//            _Bool                  masked);

//      struct ggml_tensor * ggml_flash_ff(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * b0,
//             struct ggml_tensor  * b1,
//             struct ggml_tensor  * c0,
//             struct ggml_tensor  * c1);

//     // partition into non-overlapping windows with padding if needed
//     // example:
//     // a:   768   64   64    1
//     // w:    14
//     // res: 768   14   14    25
//     // used in sam
//      struct ggml_tensor * ggml_win_part(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int                   w);

//     // reverse of ggml_win_part
//     // used in sam
//      struct ggml_tensor * ggml_win_unpart(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int                   w0,
//             int                   h0,
//             int                   w);

//      struct ggml_tensor * ggml_unary(
//             struct ggml_context * ctx,
//              struct ggml_tensor * a,
//              enum ggml_unary_op op);

//      struct ggml_tensor * ggml_unary_inplace(
//         struct ggml_context * ctx,
//         struct ggml_tensor  * a,
//         enum ggml_unary_op op);

//     // used in sam
//      struct ggml_tensor * ggml_get_rel_pos(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             int                   qh,
//             int                   kh);

//     // used in sam
//      struct ggml_tensor * ggml_add_rel_pos(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * pw,
//             struct ggml_tensor  * ph);

//      struct ggml_tensor * ggml_add_rel_pos_inplace(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * a,
//             struct ggml_tensor  * pw,
//             struct ggml_tensor  * ph);

//     // custom operators

//     typedef void (*ggml_unary_op_f32_t) (const int, float *, const float *);
//     typedef void (*ggml_binary_op_f32_t)(const int, float *, const float *, const float *);

//     typedef void (*ggml_custom1_op_f32_t)(struct ggml_tensor *, const struct ggml_tensor *);
//     typedef void (*ggml_custom2_op_f32_t)(struct ggml_tensor *, const struct ggml_tensor *, const struct ggml_tensor *);
//     typedef void (*ggml_custom3_op_f32_t)(struct ggml_tensor *, const struct ggml_tensor *, const struct ggml_tensor *, const struct ggml_tensor *);

//     struct ggml_tensor * ggml_map_unary_f32( struct ggml_context * ctx, struct ggml_tensor * a, ggml_unary_op_f32_t fun);

//     struct ggml_tensor * ggml_map_unary_inplace_f32( struct ggml_context * ctx, struct ggml_tensor * a, ggml_unary_op_f32_t fun);

//     struct ggml_tensor * ggml_map_binary_f32( struct ggml_context * ctx, struct ggml_tensor * a, struct ggml_tensor * b, ggml_binary_op_f32_t fun);

//     struct ggml_tensor * ggml_map_binary_inplace_f32( struct ggml_context * ctx, struct ggml_tensor * a, struct ggml_tensor * b, ggml_binary_op_f32_t fun);

//     struct ggml_tensor * ggml_map_custom1_f32( struct ggml_context * ctx, struct ggml_tensor * a, ggml_custom1_op_f32_t fun);

//     struct ggml_tensor * ggml_map_custom1_inplace_f32( struct ggml_context * ctx, struct ggml_tensor * a, ggml_custom1_op_f32_t fun);

//     struct ggml_tensor * ggml_map_custom2_f32( struct ggml_context * ctx, struct ggml_tensor * a, struct ggml_tensor * b, ggml_custom2_op_f32_t fun);

//     struct ggml_tensor * ggml_map_custom2_inplace_f32( struct ggml_context * ctx, struct ggml_tensor * a, struct ggml_tensor * b, ggml_custom2_op_f32_t fun);

//     struct ggml_tensor * ggml_map_custom3_f32( struct ggml_context * ctx, struct ggml_tensor * a, struct ggml_tensor * b, struct ggml_tensor * c, ggml_custom3_op_f32_t fun);

//     struct ggml_tensor * ggml_map_custom3_inplace_f32( struct ggml_context * ctx, struct ggml_tensor * a, struct ggml_tensor * b, struct ggml_tensor * c, ggml_custom3_op_f32_t fun);

//     // custom operators v2

//     typedef void (*ggml_custom1_op_t)(struct ggml_tensor * dst , const struct ggml_tensor * a, int ith, int nth, void * userdata);
//     typedef void (*ggml_custom2_op_t)(struct ggml_tensor * dst , const struct ggml_tensor * a, const struct ggml_tensor * b, int ith, int nth, void * userdata);
//     typedef void (*ggml_custom3_op_t)(struct ggml_tensor * dst , const struct ggml_tensor * a, const struct ggml_tensor * b, const struct ggml_tensor * c, int ith, int nth, void * userdata);

//     // llamacpp.h GGML_N_TASKS_MAX -1

//      struct ggml_tensor * ggml_map_custom1(
//             struct ggml_context   * ctx,
//             struct ggml_tensor    * a,
//             ggml_custom1_op_t       fun,
//             int                     n_tasks,
//             void                  * userdata);

//      struct ggml_tensor * ggml_map_custom1_inplace(
//             struct ggml_context   * ctx,
//             struct ggml_tensor    * a,
//             ggml_custom1_op_t       fun,
//             int                     n_tasks,
//             void                  * userdata);

//      struct ggml_tensor * ggml_map_custom2(
//             struct ggml_context   * ctx,
//             struct ggml_tensor    * a,
//             struct ggml_tensor    * b,
//             ggml_custom2_op_t       fun,
//             int                     n_tasks,
//             void                  * userdata);

//      struct ggml_tensor * ggml_map_custom2_inplace(
//             struct ggml_context   * ctx,
//             struct ggml_tensor    * a,
//             struct ggml_tensor    * b,
//             ggml_custom2_op_t       fun,
//             int                     n_tasks,
//             void                  * userdata);

//      struct ggml_tensor * ggml_map_custom3(
//             struct ggml_context   * ctx,
//             struct ggml_tensor    * a,
//             struct ggml_tensor    * b,
//             struct ggml_tensor    * c,
//             ggml_custom3_op_t       fun,
//             int                     n_tasks,
//             void                  * userdata);

//      struct ggml_tensor * ggml_map_custom3_inplace(
//             struct ggml_context   * ctx,
//             struct ggml_tensor    * a,
//             struct ggml_tensor    * b,
//             struct ggml_tensor    * c,
//             ggml_custom3_op_t       fun,
//             int                     n_tasks,
//             void                  * userdata);

//     // loss function

//      struct ggml_tensor * ggml_cross_entropy_loss(
//             struct ggml_context         * ctx,
//             struct ggml_tensor          * a,
//             struct ggml_tensor          * b);

//      struct ggml_tensor * ggml_cross_entropy_loss_back(
//             struct ggml_context         * ctx,
//             struct ggml_tensor          * a,
//             struct ggml_tensor          * b,
//             struct ggml_tensor          * c);

//     //
//     // automatic differentiation
//     //

//      void ggml_set_param(
//             struct ggml_context * ctx,
//             struct ggml_tensor  * tensor);


//      void ggml_build_forward_expand (struct ggml_cgraph * cgraph, struct ggml_tensor * tensor);
//      void ggml_build_backward_expand(struct ggml_context * ctx, struct ggml_cgraph * gf, struct ggml_cgraph * gb, _Bool keep);

//     // graph allocation in a context
//      struct ggml_cgraph * ggml_new_graph         (struct ggml_context * ctx); // size = GGML_DEFAULT_GRAPH_SIZE, grads = false
//      struct ggml_cgraph * ggml_new_graph_custom  (struct ggml_context * ctx, size_t size, _Bool grads);
//      struct ggml_cgraph * ggml_graph_dup         (struct ggml_context * ctx, struct ggml_cgraph * cgraph);
//      struct ggml_cgraph   ggml_graph_view        (struct ggml_cgraph * cgraph, int i0, int i1);
//      void                 ggml_graph_cpy         (struct ggml_cgraph * src, struct ggml_cgraph * dst);
//      void                 ggml_graph_reset       (struct ggml_cgraph * cgraph);  // zero grads
//      void                 ggml_graph_clear       (struct ggml_cgraph * cgraph);

//      size_t ggml_graph_overhead(void);
//      size_t ggml_graph_overhead_custom(size_t size, _Bool grads);

//     // ggml_graph_plan() has to be called before ggml_graph_compute()
//     // when plan.work_size > 0, caller must allocate memory for plan.work_data
//      struct ggml_cplan ggml_graph_plan   (const struct ggml_cgraph * cgraph, int n_threads /*= GGML_DEFAULT_N_THREADS*/);
//      int               ggml_graph_compute(      struct ggml_cgraph * cgraph, struct ggml_cplan * cplan);

//     // same as ggml_graph_compute() but the work data is allocated as a part of the context
//     // note: the drawback of this API is that you must have ensured that the context has enough memory for the work data
//      void ggml_graph_compute_with_ctx(struct ggml_context * ctx, struct ggml_cgraph * cgraph, int n_threads);

//      struct ggml_tensor * ggml_graph_get_tensor(struct ggml_cgraph * cgraph, const char * name);

//      void                 ggml_graph_export(const struct ggml_cgraph * cgraph, const char * fname);
//      struct ggml_cgraph * ggml_graph_import(const char * fname, struct ggml_context ** ctx_data, struct ggml_context ** ctx_eval);

//     // print info and performance information for the graph
//      void ggml_graph_print(const struct ggml_cgraph * cgraph);

//     // dump the graph into a file using the dot format
//      void ggml_graph_dump_dot(const struct ggml_cgraph * gb, const struct ggml_cgraph * gf, const char * filename);

//     // build gradient checkpointing backward graph gb for gf using provided checkpoints
//     // gb_tmp will contain original backward graph with rewritten backward process nodes,
//     // but without the second forward pass nodes.
//      void ggml_build_backward_gradient_checkpointing(
//             struct ggml_context   * ctx,
//             struct ggml_cgraph    * gf,
//             struct ggml_cgraph    * gb,
//             struct ggml_cgraph    * gb_tmp,
//             struct ggml_tensor  * * checkpoints,
//             int                     n_checkpoints);
//     //
//     // optimization
//     //

//     // optimization methods
//     enum ggml_opt_type {
//         GGML_OPT_ADAM,
//         GGML_OPT_LBFGS,
//     };

//     // linesearch methods
//     enum ggml_linesearch {
//         GGML_LINESEARCH_DEFAULT = 1,

//         GGML_LINESEARCH_BACKTRACKING_ARMIJO       = 0,
//         GGML_LINESEARCH_BACKTRACKING_WOLFE        = 1,
//         GGML_LINESEARCH_BACKTRACKING_STRONG_WOLFE = 2,
//     };

//     // optimization return values
//     enum ggml_opt_result {
//         GGML_OPT_OK = 0,
//         GGML_OPT_DID_NOT_CONVERGE,
//         GGML_OPT_NO_CONTEXT,
//         GGML_OPT_INVALID_WOLFE,
//         GGML_OPT_FAIL,
//         GGML_OPT_CANCEL,

//         GGML_LINESEARCH_FAIL = -128,
//         GGML_LINESEARCH_MINIMUM_STEP,
//         GGML_LINESEARCH_MAXIMUM_STEP,
//         GGML_LINESEARCH_MAXIMUM_ITERATIONS,
//         GGML_LINESEARCH_INVALID_PARAMETERS,
//     };

//     typedef void (*ggml_opt_callback)(void * data, int accum_step, float * sched, _Bool * cancel);
//     typedef void (*ggml_log_callback)(enum ggml_log_level level, const char * text, void * user_data);

//     // optimization parameters
//     //
//     //   see ggml.c (ggml_opt_default_params) for default values
//     //
//     struct ggml_opt_params {
//         enum ggml_opt_type type;

//         size_t graph_size;

//         int n_threads;

//         // delta-based convergence test
//         //
//         //   if past == 0 - disabled
//         //   if past > 0:
//         //     stop if |f(x) - f(x_past)| < delta * max(1, |f(x)|)
//         //
//         int past;
//         float delta;

//         // maximum number of iterations without improvement
//         //
//         //   if 0 - disabled
//         //   if > 0:
//         //     assume convergence if no cost improvement in this number of iterations
//         //
//         int max_no_improvement;

//         _Bool print_forward_graph;
//         _Bool print_backward_graph;

//         int n_gradient_accumulation;

//         // ADAM parameters
//         struct {
//             int n_iter;

//             float sched; // schedule multiplier (fixed, decay or warmup)
//             float decay; // weight decay for AdamW, use 0.0f to disable
//             int   decay_min_ndim; // minimum number of tensor dimension to apply weight decay
//             float alpha; // learning rate
//             float beta1;
//             float beta2;
//             float eps;   // epsilon for numerical stability
//             float eps_f; // epsilon for convergence test
//             float eps_g; // epsilon for convergence test
//             float gclip; // gradient clipping
//         } adam;

//         // LBFGS parameters
//         struct {
//             int m; // number of corrections to approximate the inv. Hessian
//             int n_iter;
//             int max_linesearch;

//             float eps;      // convergence tolerance
//             float ftol;     // line search tolerance
//             float wolfe;
//             float min_step;
//             float max_step;

//             enum ggml_linesearch linesearch;
//         } lbfgs;
//     };

//     struct ggml_opt_context {
//         struct ggml_context * ctx;
//         struct ggml_opt_params params;

//         int iter;
//         int64_t nx; // number of parameter elements

//         _Bool just_initialized;

//         float loss_before;
//         float loss_after;

//         struct {
//             struct ggml_tensor * g;  // current gradient
//             struct ggml_tensor * m;  // first moment
//             struct ggml_tensor * v;  // second moment
//             struct ggml_tensor * pf; // past function values
//             float fx_best;
//             float fx_prev;
//             int n_no_improvement;
//         } adam;

//         struct {
//             struct ggml_tensor * x;    // current parameters
//             struct ggml_tensor * xp;   // previous parameters
//             struct ggml_tensor * g;    // current gradient
//             struct ggml_tensor * gp;   // previous gradient
//             struct ggml_tensor * d;    // search direction
//             struct ggml_tensor * pf;   // past function values
//             struct ggml_tensor * lmal; // the L-BFGS memory alpha
//             struct ggml_tensor * lmys; // the L-BFGS memory ys
//             struct ggml_tensor * lms;  // the L-BFGS memory s
//             struct ggml_tensor * lmy;  // the L-BFGS memory y
//             float fx_best;
//             float step;
//             int j;
//             int k;
//             int end;
//             int n_no_improvement;
//         } lbfgs;
//     };

//      struct ggml_opt_params ggml_opt_default_params(enum ggml_opt_type type);

//     // optimize the function defined by the tensor f
//      enum ggml_opt_result ggml_opt(
//             struct ggml_context * ctx,
//             struct ggml_opt_params params,
//             struct ggml_tensor * f);

//     // initialize optimizer context
//      void ggml_opt_init(
//             struct ggml_context     * ctx,
//             struct ggml_opt_context * opt,
//             struct ggml_opt_params    params,
//             int64_t                   nx);

//     // continue optimizing the function defined by the tensor f
//      enum ggml_opt_result ggml_opt_resume(
//             struct ggml_context * ctx,
//             struct ggml_opt_context * opt,
//             struct ggml_tensor * f);

//     // continue optimizing the function defined by the tensor f
//      enum ggml_opt_result ggml_opt_resume_g(
//             struct ggml_context * ctx,
//             struct ggml_opt_context * opt,
//             struct ggml_tensor * f,
//             struct ggml_cgraph * gf,
//             struct ggml_cgraph * gb,
//             ggml_opt_callback callback,
//             void * callback_data);

//     //
//     // tensor flags
//     //
//      void ggml_set_input(struct ggml_tensor * tensor);
//      void ggml_set_output(struct ggml_tensor * tensor);

//     //
//     // quantization
//     //

//     // - ggml_quantize_init can be called multiple times with the same type
//     //   it will only initialize the quantization tables for the first call or after ggml_quantize_free
//     //   automatically called by ggml_quantize_chunk for convenience
//     //
//     // - ggml_quantize_free will free any memory allocated by ggml_quantize_init
//     //   call this at the end of the program to avoid memory leaks
//     //
//     // note: these are thread-safe
//     //
//      void ggml_quantize_init(enum ggml_type type);
//      void ggml_quantize_free(void);

//     // TODO: these would probably get removed in favor of the more general ggml_quantize_chunk
//      size_t ggml_quantize_q4_0(const float * src, void * dst, int n, int k, int64_t * hist);
//      size_t ggml_quantize_q4_1(const float * src, void * dst, int n, int k, int64_t * hist);
//      size_t ggml_quantize_q5_0(const float * src, void * dst, int n, int k, int64_t * hist);
//      size_t ggml_quantize_q5_1(const float * src, void * dst, int n, int k, int64_t * hist);
//      size_t ggml_quantize_q8_0(const float * src, void * dst, int n, int k, int64_t * hist);

//      size_t ggml_quantize_q2_K(const float * src, void * dst, int n, int k, int64_t * hist);
//      size_t ggml_quantize_q3_K(const float * src, void * dst, int n, int k, int64_t * hist);
//      size_t ggml_quantize_q4_K(const float * src, void * dst, int n, int k, int64_t * hist);
//      size_t ggml_quantize_q5_K(const float * src, void * dst, int n, int k, int64_t * hist);
//      size_t ggml_quantize_q6_K(const float * src, void * dst, int n, int k, int64_t * hist);

//     // some quantization type cannot be used without an importance matrix
//      _Bool ggml_quantize_requires_imatrix(enum ggml_type type);

//     // calls ggml_quantize_init internally (i.e. can allocate memory)
//      size_t ggml_quantize_chunk(enum ggml_type type, const float * src, void * dst,
//             int start, int nrows, int n_per_row, int64_t * hist, const float * imatrix);

//     //
//     // gguf
//     //

//     enum gguf_type {
//         GGUF_TYPE_UINT8   = 0,
//         GGUF_TYPE_INT8    = 1,
//         GGUF_TYPE_UINT16  = 2,
//         GGUF_TYPE_INT16   = 3,
//         GGUF_TYPE_UINT32  = 4,
//         GGUF_TYPE_INT32   = 5,
//         GGUF_TYPE_FLOAT32 = 6,
//         GGUF_TYPE_BOOL    = 7,
//         GGUF_TYPE_STRING  = 8,
//         GGUF_TYPE_ARRAY   = 9,
//         GGUF_TYPE_UINT64  = 10,
//         GGUF_TYPE_INT64   = 11,
//         GGUF_TYPE_FLOAT64 = 12,
//         GGUF_TYPE_COUNT,       // marks the end of the enum
//     };

//     struct gguf_context;

//     struct gguf_init_params {
//         _Bool no_alloc;

//         // if not NULL, create a ggml_context and allocate the tensor data in it
//         struct ggml_context ** ctx;
//     };

//      struct gguf_context * gguf_init_empty(void);
//      struct gguf_context * gguf_init_from_file(const char * fname, struct gguf_init_params params);
//     //GGML_API struct gguf_context * gguf_init_from_buffer(..);

//      void gguf_free(struct gguf_context * ctx);

//      const char * gguf_type_name(enum gguf_type type);

//      int    gguf_get_version    (const struct gguf_context * ctx);
//      size_t gguf_get_alignment  (const struct gguf_context * ctx);
//      size_t gguf_get_data_offset(const struct gguf_context * ctx);
//      void * gguf_get_data       (const struct gguf_context * ctx);

//      int          gguf_get_n_kv(const struct gguf_context * ctx);
//      int          gguf_find_key(const struct gguf_context * ctx, const char * key);
//      const char * gguf_get_key (const struct gguf_context * ctx, int key_id);

//      enum gguf_type gguf_get_kv_type (const struct gguf_context * ctx, int key_id);
//      enum gguf_type gguf_get_arr_type(const struct gguf_context * ctx, int key_id);

//     // will abort if the wrong type is used for the key
//      uint8_t      gguf_get_val_u8  (const struct gguf_context * ctx, int key_id);
//      int8_t       gguf_get_val_i8  (const struct gguf_context * ctx, int key_id);
//      uint16_t     gguf_get_val_u16 (const struct gguf_context * ctx, int key_id);
//      int16_t      gguf_get_val_i16 (const struct gguf_context * ctx, int key_id);
//      uint32_t     gguf_get_val_u32 (const struct gguf_context * ctx, int key_id);
//      int32_t      gguf_get_val_i32 (const struct gguf_context * ctx, int key_id);
//      float        gguf_get_val_f32 (const struct gguf_context * ctx, int key_id);
//      uint64_t     gguf_get_val_u64 (const struct gguf_context * ctx, int key_id);
//      int64_t      gguf_get_val_i64 (const struct gguf_context * ctx, int key_id);
//      double       gguf_get_val_f64 (const struct gguf_context * ctx, int key_id);
//      _Bool         gguf_get_val_bool(const struct gguf_context * ctx, int key_id);
//      const char * gguf_get_val_str (const struct gguf_context * ctx, int key_id);
//      const void * gguf_get_val_data(const struct gguf_context * ctx, int key_id);
//      int          gguf_get_arr_n   (const struct gguf_context * ctx, int key_id);
//      const void * gguf_get_arr_data(const struct gguf_context * ctx, int key_id);
//      const char * gguf_get_arr_str (const struct gguf_context * ctx, int key_id, int i);

//      int            gguf_get_n_tensors    (const struct gguf_context * ctx);
//      int            gguf_find_tensor      (const struct gguf_context * ctx, const char * name);
//      size_t         gguf_get_tensor_offset(const struct gguf_context * ctx, int i);
//      char *         gguf_get_tensor_name  (const struct gguf_context * ctx, int i);
//      enum ggml_type gguf_get_tensor_type  (const struct gguf_context * ctx, int i);

//     // overrides existing values or adds a new one
//      void gguf_set_val_u8  (struct gguf_context * ctx, const char * key, uint8_t  val);
//      void gguf_set_val_i8  (struct gguf_context * ctx, const char * key, int8_t   val);
//      void gguf_set_val_u16 (struct gguf_context * ctx, const char * key, uint16_t val);
//      void gguf_set_val_i16 (struct gguf_context * ctx, const char * key, int16_t  val);
//      void gguf_set_val_u32 (struct gguf_context * ctx, const char * key, uint32_t val);
//      void gguf_set_val_i32 (struct gguf_context * ctx, const char * key, int32_t  val);
//      void gguf_set_val_f32 (struct gguf_context * ctx, const char * key, float    val);
//      void gguf_set_val_u64 (struct gguf_context * ctx, const char * key, uint64_t val);
//      void gguf_set_val_i64 (struct gguf_context * ctx, const char * key, int64_t  val);
//      void gguf_set_val_f64 (struct gguf_context * ctx, const char * key, double   val);
//      void gguf_set_val_bool(struct gguf_context * ctx, const char * key, _Bool     val);
//      void gguf_set_val_str (struct gguf_context * ctx, const char * key, const char * val);
//      void gguf_set_arr_data(struct gguf_context * ctx, const char * key, enum gguf_type type, const void * data, int n);
//      void gguf_set_arr_str (struct gguf_context * ctx, const char * key, const char ** data, int n);

//     // set or add KV pairs from another context
//      void gguf_set_kv(struct gguf_context * ctx, struct gguf_context * src);

//     // manage tensor info
//      void gguf_add_tensor(struct gguf_context * ctx, const struct ggml_tensor * tensor);
//      void gguf_set_tensor_type(struct gguf_context * ctx, const char * name, enum ggml_type type);
//      void gguf_set_tensor_data(struct gguf_context * ctx, const char * name, const void * data, size_t size);

//     // writing gguf files can be done in 2 ways:
//     //
//     // - write the entire gguf_context to a binary file in a single pass:
//     //
//     //   gguf_write_to_file(ctx, fname);
//     //
//     // - first prepare a file with a placeholder for the meta data, write the tensor data, then write the meta data:
//     //
//     //   FILE * f = fopen(fname, "wb");
//     //   fseek(f, gguf_get_meta_size(ctx), SEEK_SET);
//     //   fwrite(f, ...);
//     //   void * data = gguf_meta_get_meta_data(ctx);
//     //   fseek(f, 0, SEEK_SET);
//     //   fwrite(f, data, gguf_get_meta_size(ctx));
//     //   free(data);
//     //   fclose(f);
//     //

//     // write the entire context to a binary file
//      void gguf_write_to_file(const struct gguf_context * ctx, const char * fname, _Bool only_meta);

//     // get the size in bytes of the meta data (header, kv pairs, tensor info) including padding
//      size_t gguf_get_meta_size(const struct gguf_context * ctx);
//      void   gguf_get_meta_data(const struct gguf_context * ctx, void * data);

//     //
//     // system info
//     //

//      int ggml_cpu_has_avx        (void);
//      int ggml_cpu_has_avx_vnni   (void);
//      int ggml_cpu_has_avx2       (void);
//      int ggml_cpu_has_avx512     (void);
//      int ggml_cpu_has_avx512_vbmi(void);
//      int ggml_cpu_has_avx512_vnni(void);
//      int ggml_cpu_has_fma        (void);
//      int ggml_cpu_has_neon       (void);
//      int ggml_cpu_has_arm_fma    (void);
//      int ggml_cpu_has_metal      (void);
//      int ggml_cpu_has_f16c       (void);
//      int ggml_cpu_has_fp16_va    (void);
//      int ggml_cpu_has_wasm_simd  (void);
//      int ggml_cpu_has_blas       (void);
//      int ggml_cpu_has_cublas     (void);
//      int ggml_cpu_has_clblast    (void);
//      int ggml_cpu_has_vulkan     (void);
//      int ggml_cpu_has_kompute    (void);
//      int ggml_cpu_has_gpublas    (void);
//      int ggml_cpu_has_sse3       (void);
//      int ggml_cpu_has_ssse3      (void);
//      int ggml_cpu_has_sycl       (void);
//      int ggml_cpu_has_vsx        (void);
//      int ggml_cpu_has_matmul_int8(void);

//     //
//     // Internal types and functions exposed for tests and benchmarks
//     //


// // llamacpp.h GGML_RESTRICT restrict

//     typedef void (*ggml_to_float_t)  (const void  * restrict x, float * restrict y, int k);
//     typedef void (*ggml_from_float_t)(const float * restrict x, void  * restrict y, int k);
//     typedef void (*ggml_vec_dot_t)   (int n, float * restrict s, size_t bs, const void * restrict x, size_t bx,
//                                       const void * restrict y, size_t by, int nrc);

//     typedef struct {
//         const char      * type_name;
//         int               blck_size;
//         size_t            type_size;
//         _Bool              is_quantized;
//         ggml_to_float_t   to_float;
//         ggml_from_float_t from_float;
//         ggml_from_float_t from_float_reference;
//         ggml_vec_dot_t    vec_dot;
//         enum ggml_type    vec_dot_type;
//         int64_t           nrows; // number of rows to process simultaneously;
//     } ggml_type_traits_t;

//      ggml_type_traits_t ggml_internal_get_type_traits(enum ggml_type type);




// typedef struct ggml_backend_buffer_type * ggml_backend_buffer_type_t;
// typedef struct ggml_backend_buffer * ggml_backend_buffer_t;
// typedef struct ggml_backend * ggml_backend_t;

// // Tensor allocator
// typedef struct ggml_tallocr * ggml_tallocr_t;

//  ggml_tallocr_t ggml_tallocr_new(ggml_backend_buffer_t buffer);
//  void           ggml_tallocr_free(ggml_tallocr_t talloc);
//  void           ggml_tallocr_alloc(ggml_tallocr_t talloc, struct ggml_tensor * tensor);

// // Graph allocator
// /*
//   Example usage:
//     ggml_gallocr_t galloc = ggml_gallocr_new(ggml_bacckend_cpu_buffer_type());

//     // optional: create a worst-case graph and reserve the buffers to avoid reallocations
//     ggml_gallocr_reserve(galloc, build_graph(max_batch));

//     // allocate the graph
//     struct ggml_cgraph * graph = build_graph(batch);
//     ggml_gallocr_alloc_graph(galloc, graph);

//     printf("compute buffer size: %zu bytes\n", ggml_gallocr_get_buffer_size(galloc, 0));

//     // evaluate the graph
//     ggml_backend_graph_compute(backend, graph);
// */

// // special tensor flags for use with the graph allocator:
// //   ggml_set_input(): all input tensors are allocated at the beginning of the graph in non-overlapping addresses
// //   ggml_set_output(): output tensors are never freed and never overwritten

// typedef struct ggml_gallocr * ggml_gallocr_t;

//  ggml_gallocr_t ggml_gallocr_new(ggml_backend_buffer_type_t buft);
//  ggml_gallocr_t ggml_gallocr_new_n(ggml_backend_buffer_type_t * bufts, int n_bufs);
//  void           ggml_gallocr_free(ggml_gallocr_t galloc);

// // pre-allocate buffers from a measure graph - does not allocate or modify the graph
// // call with a worst-case graph to avoid buffer reallocations
// // not strictly required for single buffer usage: ggml_gallocr_alloc_graph will reallocate the buffers automatically if needed
// // returns false if the buffer allocation failed
//  _Bool ggml_gallocr_reserve(ggml_gallocr_t galloc, struct ggml_cgraph * graph);
//  _Bool ggml_gallocr_reserve_n(ggml_gallocr_t galloc, struct ggml_cgraph * graph, const int * node_buffer_ids);

// // automatic reallocation if the topology changes when using a single buffer
// // returns false if using multiple buffers and a re-allocation is needed (call ggml_gallocr_reserve_n first to set the node buffers)
//  _Bool ggml_gallocr_alloc_graph(ggml_gallocr_t galloc, struct ggml_cgraph * graph);

//  size_t ggml_gallocr_get_buffer_size(ggml_gallocr_t galloc, int buffer_id);

// // Utils
// // Create a buffer and allocate all the tensors in a ggml_context
//  struct ggml_backend_buffer * ggml_backend_alloc_ctx_tensors_from_buft(struct ggml_context * ctx, ggml_backend_buffer_type_t buft);
//  struct ggml_backend_buffer * ggml_backend_alloc_ctx_tensors(struct ggml_context * ctx, ggml_backend_t backend);



//     typedef struct ggml_backend_buffer_type * ggml_backend_buffer_type_t;
//     typedef struct ggml_backend_buffer * ggml_backend_buffer_t;
//     typedef struct ggml_backend * ggml_backend_t;
//     typedef void * ggml_backend_graph_plan_t;

//     //
//     // Backend buffer
//     //

//     // buffer type
//                const char *          ggml_backend_buft_name            (ggml_backend_buffer_type_t buft);
//       ggml_backend_buffer_t ggml_backend_buft_alloc_buffer    (ggml_backend_buffer_type_t buft, size_t size);
//                size_t                ggml_backend_buft_get_alignment   (ggml_backend_buffer_type_t buft);
//                size_t                ggml_backend_buft_get_max_size    (ggml_backend_buffer_type_t buft);
//       size_t                ggml_backend_buft_get_alloc_size  (ggml_backend_buffer_type_t buft, struct ggml_tensor * tensor);
//                _Bool                  ggml_backend_buft_supports_backend(ggml_backend_buffer_type_t buft, ggml_backend_t backend);
//                _Bool                  ggml_backend_buft_is_host         (ggml_backend_buffer_type_t buft);

//     // buffer
//     enum ggml_backend_buffer_usage {
//         GGML_BACKEND_BUFFER_USAGE_ANY = 0,
//         GGML_BACKEND_BUFFER_USAGE_WEIGHTS = 1,
//     };

//                const char *               ggml_backend_buffer_name          (ggml_backend_buffer_t buffer);
//                void                       ggml_backend_buffer_free          (ggml_backend_buffer_t buffer);
//                void *                     ggml_backend_buffer_get_base      (ggml_backend_buffer_t buffer);
//                size_t                     ggml_backend_buffer_get_size      (ggml_backend_buffer_t buffer);
//       void                       ggml_backend_buffer_init_tensor   (ggml_backend_buffer_t buffer, struct ggml_tensor * tensor);
//                size_t                     ggml_backend_buffer_get_alignment (ggml_backend_buffer_t buffer);
//                size_t                     ggml_backend_buffer_get_max_size  (ggml_backend_buffer_t buffer);
//                size_t                     ggml_backend_buffer_get_alloc_size(ggml_backend_buffer_t buffer, struct ggml_tensor * tensor);
//                void                       ggml_backend_buffer_clear         (ggml_backend_buffer_t buffer, uint8_t value);
//                _Bool                       ggml_backend_buffer_is_host       (ggml_backend_buffer_t buffer);
//                void                       ggml_backend_buffer_set_usage     (ggml_backend_buffer_t buffer, enum ggml_backend_buffer_usage usage);
//                ggml_backend_buffer_type_t ggml_backend_buffer_get_type      (ggml_backend_buffer_t buffer);
//                void                       ggml_backend_buffer_reset         (ggml_backend_buffer_t buffer);

//     //
//     // Backend
//     //


//      const char * ggml_backend_name(ggml_backend_t backend);
//      void         ggml_backend_free(ggml_backend_t backend);

//      ggml_backend_buffer_type_t ggml_backend_get_default_buffer_type(ggml_backend_t backend);
//      ggml_backend_buffer_t      ggml_backend_alloc_buffer(ggml_backend_t backend, size_t size);
//      size_t                     ggml_backend_get_alignment(ggml_backend_t backend);
//      size_t                     ggml_backend_get_max_size(ggml_backend_t backend);

//      void ggml_backend_tensor_set_async(ggml_backend_t backend,       struct ggml_tensor * tensor, const void * data, size_t offset, size_t size);
//      void ggml_backend_tensor_get_async(ggml_backend_t backend, const struct ggml_tensor * tensor,       void * data, size_t offset, size_t size);

//       void ggml_backend_tensor_set(      struct ggml_tensor * tensor, const void * data, size_t offset, size_t size);
//       void ggml_backend_tensor_get(const struct ggml_tensor * tensor,       void * data, size_t offset, size_t size);

//      void ggml_backend_synchronize(ggml_backend_t backend);

//      ggml_backend_graph_plan_t ggml_backend_graph_plan_create (ggml_backend_t backend, struct ggml_cgraph * cgraph);

//      void ggml_backend_graph_plan_free   (ggml_backend_t backend, ggml_backend_graph_plan_t plan);
//      void ggml_backend_graph_plan_compute(ggml_backend_t backend, ggml_backend_graph_plan_t plan);
//      _Bool ggml_backend_graph_compute     (ggml_backend_t backend, struct ggml_cgraph * cgraph);
//      _Bool ggml_backend_supports_op       (ggml_backend_t backend, const struct ggml_tensor * op);

//     // tensor copy between different backends
//      void ggml_backend_tensor_copy(struct ggml_tensor * src, struct ggml_tensor * dst);
//      void ggml_backend_tensor_copy_async(ggml_backend_t backend, struct ggml_tensor * src, struct ggml_tensor * dst); // automatic fallback to sync copy

//     //
//     // CPU backend
//     //

//      ggml_backend_t ggml_backend_cpu_init(void);

//       _Bool ggml_backend_is_cpu                (ggml_backend_t backend);
//                void ggml_backend_cpu_set_n_threads     (ggml_backend_t backend_cpu, int n_threads);
//                void ggml_backend_cpu_set_abort_callback(ggml_backend_t backend_cpu, ggml_abort_callback abort_callback, void * abort_callback_data);

//     // Create a backend buffer from an existing pointer
//       ggml_backend_buffer_t ggml_backend_cpu_buffer_from_ptr(void * ptr, size_t size);

//       ggml_backend_buffer_type_t ggml_backend_cpu_buffer_type(void);




//     //
//     // Backend registry
//     //

//     // The backend registry is a registry of all the available backends, and allows initializing backends in a generic way

//      size_t                     ggml_backend_reg_get_count(void);
//      size_t                     ggml_backend_reg_find_by_name(const char * name);
//      ggml_backend_t             ggml_backend_reg_init_backend_from_str(const char * backend_str); // str is name[:params]
//      const char *               ggml_backend_reg_get_name(size_t i);
//      ggml_backend_t             ggml_backend_reg_init_backend(size_t i, const char * params); // params is backend-specific
//      ggml_backend_buffer_type_t ggml_backend_reg_get_default_buffer_type(size_t i);
//      ggml_backend_buffer_t      ggml_backend_reg_alloc_buffer(size_t i, size_t size);

//     //
//     // Backend scheduler
//     //

//     // The backend scheduler allows for multiple backends to be used together
//     // Handles compute buffer allocation, assignment of tensors to backends, and copying of tensors between backends
//     // The backends are selected based on:
//     // - the backend that supports the operation
//     // - the location of the pre-allocated tensors (e.g. the weights)
//     /*
//       Example usage:

//         sched = ggml_backend_sched_new({backend_gpu, backend_gpu2, backend_cpu}, num_backends);
//         // sched is initialized with measure allocators and cannot be used until allocated with a measure graph

//         // initialize buffers from a measure graph
//         measure_graph = build_graph(sched); // use the allocr to allocate inputs as needed

//         // in build_graph:
//         build_graph(...) {
//             // manually assign nodes to a backend (optional, should not be needed in most cases)
//             struct ggml_tensor * node = ggml_mul_mat(ctx, ...);
//             ggml_backend_sched_set_node_backend(sched, node, backend_gpu);
//         }

//         // allocate backend buffers from measure graph
//         ggml_backend_sched_init_measure(sched, measure_graph);

//         // the scheduler is now ready to compute graphs

//         // compute
//         graph = build_graph(sched);
//         ggml_backend_sched_graph_compute(sched, graph);
//     */

//     struct ggml_backend_sched;
//     typedef struct ggml_backend_sched * ggml_backend_sched_t;

//     // when ask == true, the scheduler wants to know if the user wants to observe this node
//     // this allows the scheduler to batch nodes together in order to evaluate them in a single call
//     //
//     // when ask == false, the scheduler is passing the node tensor to the user for observation
//     // if the user returns false, the scheduler will cancel the graph compute
//     //
//     typedef _Bool (*ggml_backend_sched_eval_callback)(struct ggml_tensor * t, _Bool ask, void * user_data);

//     // Initialize a backend scheduler
//      ggml_backend_sched_t  ggml_backend_sched_new(ggml_backend_t * backends, ggml_backend_buffer_type_t * bufts, int n_backends, size_t graph_size);
//      void                  ggml_backend_sched_free(ggml_backend_sched_t sched);
//     // Initialize backend buffers from a measure graph
//      _Bool                  ggml_backend_sched_reserve(ggml_backend_sched_t sched, struct ggml_cgraph * measure_graph);
//     // Get the number of splits of the last graph
//      int                   ggml_backend_sched_get_n_splits(ggml_backend_sched_t sched);

//      size_t                ggml_backend_sched_get_buffer_size(ggml_backend_sched_t sched, ggml_backend_t backend);

//      void                  ggml_backend_sched_set_node_backend(ggml_backend_sched_t sched, struct ggml_tensor * node, ggml_backend_t backend);
//      ggml_backend_t        ggml_backend_sched_get_node_backend(ggml_backend_sched_t sched, struct ggml_tensor * node);

//     // Allocate and compute graph on the backend scheduler
//      _Bool                  ggml_backend_sched_graph_compute(ggml_backend_sched_t sched, struct ggml_cgraph * graph);

//     // Reset all assignments and allocators - must be called before changing the node backends
//      void                  ggml_backend_sched_reset(ggml_backend_sched_t sched);

//     // Set a callback to be called for each resulting node during graph compute
//      void                  ggml_backend_sched_set_eval_callback(ggml_backend_sched_t sched, ggml_backend_sched_eval_callback callback, void * user_data);

//     //
//     // Utils
//     //

//     struct ggml_backend_graph_copy {
//         ggml_backend_buffer_t buffer;
//         struct ggml_context * ctx_allocated;
//         struct ggml_context * ctx_unallocated;
//         struct ggml_cgraph * graph;
//     };

//     // Copy a graph to a different backend
//      struct ggml_backend_graph_copy ggml_backend_graph_copy(ggml_backend_t backend, struct ggml_cgraph * graph);
//      void                           ggml_backend_graph_copy_free(struct ggml_backend_graph_copy copy);

//     typedef _Bool (* ggml_backend_eval_callback)(int node_index, struct ggml_tensor * t1, struct ggml_tensor * t2, void * user_data);

//     // Compare the output of two backends
//      _Bool ggml_backend_compare_graph_backend(ggml_backend_t backend1, ggml_backend_t backend2, struct ggml_cgraph * graph, ggml_backend_eval_callback callback, void * user_data);

//     // Tensor initialization
//      void ggml_backend_tensor_alloc(ggml_backend_buffer_t buffer, struct ggml_tensor * tensor, void * addr);
//      void ggml_backend_view_init(ggml_backend_buffer_t buffer, struct ggml_tensor * tensor);








// /* Copyright (C) 1986-2001 by Digital Mars. $Revision: 1.1.1.2 $ */







// /* Define _CRTAPI1 (for compatibility with the NT SDK) */




// /* Define _CRTAPI2 (for compatibility with the NT SDK) */



// /* Define CRTIMP */








// // llamacpp.h SEEK_SET	0
// // llamacpp.h SEEK_CUR	1
// // llamacpp.h SEEK_END	2




// // llamacpp.h __RESTRICT restrict



// // llamacpp.h BUFSIZ		0x4000






// typedef unsigned size_t;




// // llamacpp.h EOF (-1)




// typedef struct _iobuf
// {

// 	char	*_ptr;
// 	int	_cnt;
// 	char	*_base;
// 	int	_flag;
// 	int	_file;
// 	int	_charbuf;
// 	int	_bufsiz;
// 	int	__tmpnum;
// // llamacpp.h _bufsize(f) ((f)->_bufsiz)


// } FILE;
// #pragma pack()


// // llamacpp.h _F_RDWR 0x0003
// // llamacpp.h _F_READ 0x0001
// // llamacpp.h _F_WRIT 0x0002
// // llamacpp.h _F_BUF  0x0004
// // llamacpp.h _F_LBUF 0x0008
// // llamacpp.h _F_ERR  0x0010
// // llamacpp.h _F_EOF  0x0020
// // llamacpp.h _F_BIN  0x0040
// // llamacpp.h _F_IN   0x0080
// // llamacpp.h _F_OUT  0x0100
// // llamacpp.h _F_TERM 0x0200






// extern	FILE __cdecl _iob[60];



// // llamacpp.h _IOREAD		1
// // llamacpp.h _IOWRT		2
// // llamacpp.h _IONBF		4
// // llamacpp.h _IOMYBUF	8
// // llamacpp.h _IOEOF		0x10
// // llamacpp.h _IOERR		0x20
// // llamacpp.h _IOLBF		0x40
// // llamacpp.h _IOSTRG         0x40
// // llamacpp.h _IORW		0x80
// // llamacpp.h _IOFBF		0
// // llamacpp.h _IOAPP		0x200

// // llamacpp.h _IOTRAN		0x100



// // llamacpp.h stdin	(&_iob[0])
// // llamacpp.h stdout	(&_iob[1])
// // llamacpp.h stderr	(&_iob[2])


// // llamacpp.h FOPEN_MAX	20



// // llamacpp.h P_tmpdir	_P_tmpdir
// // llamacpp.h wP_tmpdir	_wP_tmpdir

// // llamacpp.h TMP_MAX		32767
// // llamacpp.h _SYS_OPEN       20
// // llamacpp.h SYS_OPEN        _SYS_OPEN

// typedef long fpos_t;

// typedef char *va_list;

// int	__cdecl fwide(FILE *stream, int mode);
// char *	__cdecl tmpnam(char *);
// FILE *	__cdecl fopen(const char *,const char *);
// FILE *	__cdecl _fsopen(const char *,const char *,int );
// FILE *	__cdecl freopen(const char *,const char *,FILE *);
// int	__cdecl fseek(FILE *,long,int);
// long	__cdecl ftell(FILE *);
// char *	__cdecl fgets(char *,int,FILE *);
// int	__cdecl fgetc(FILE *);
// int   __cdecl _fgetchar(void);
// int	__cdecl fflush(FILE *);
// int	__cdecl fclose(FILE *);
// int	__cdecl fputs(const char *,FILE *);
// int	__cdecl getc(FILE *);
// int	__cdecl getchar(void);
// char *	__cdecl gets(char *);
// int	__cdecl fputc(int,FILE *);
// int   __cdecl _fputchar(int);
// int	__cdecl putc(int,FILE *);
// int	__cdecl putchar(int);
// int	__cdecl puts(const char *);
// int	__cdecl ungetc(int,FILE *);
// size_t	__cdecl fread(void *,size_t,size_t,FILE *);
// size_t	__cdecl fwrite(const void *,size_t,size_t,FILE *);
// int	__cdecl printf(const char *,...);
// int	__cdecl fprintf(FILE *,const char *,...);
// int	__cdecl  vfprintf(FILE *,const char *,va_list);
// int	__cdecl  vprintf(const char *,va_list);
// int	__cdecl sprintf(char *,const char *,...);
// int	__cdecl  vsprintf(char *,const char *,va_list);
// int	__cdecl scanf(const char *,...);
// int	__cdecl fscanf(FILE *,const char *,...);
// int	__cdecl sscanf(const char *,const char *,...);
// int	__cdecl vsnprintf(char * restrict,size_t,const char * restrict,va_list);
// int	__cdecl vscanf(const char * restrict, va_list);
// int	__cdecl vfscanf(FILE * restrict, const char * restrict, va_list);
// int	__cdecl vsscanf(const char * restrict, const char * restrict, va_list);
// void	__cdecl setbuf(FILE *,char *);
// int	__cdecl setvbuf(FILE *,char *,int,size_t);
// int	__cdecl remove(const char *);
// int	__cdecl rename(const char *,const char *);
// void	__cdecl rewind(FILE *);
// void	__cdecl clearerr(FILE *);
// int	__cdecl feof(FILE *);
// int	__cdecl ferror(FILE *);
// void	__cdecl perror(const char *);
// int	__cdecl fgetpos(FILE *,fpos_t *);
// int	__cdecl fsetpos(FILE *,const fpos_t *);
// FILE *	__cdecl tmpfile(void);
// int	__cdecl _rmtmp(void);
// int     __cdecl _fillbuf(FILE *);
// int     __cdecl _flushbu(int, FILE *);
// /*// llamacpp.h _filbuf _fillbuf*/
// /*// llamacpp.h _flsbuf _flushbu*/

// int __cdecl getw(FILE *FHdl);
// // llamacpp.h _getw  getw
// int __cdecl putw(int Word, FILE *FilePtr);
// // llamacpp.h _putw putw


// // llamacpp.h getchar()	getc(stdin)
// // llamacpp.h putchar(c)      putc(c,stdout)


// int __cdecl putch(int);
// // llamacpp.h _putch          putch


// // llamacpp.h getc(fp)	fgetc(fp)
// // llamacpp.h putc(c,fp)	fputc((c),(fp))
// // llamacpp.h ferror(fp)	((fp)->_flag&_IOERR)
// // llamacpp.h feof(fp)	((fp)->_flag&_IOEOF)
// // llamacpp.h clearerr(fp)	((void)((fp)->_flag&=~(_IOERR|_IOEOF)))
// // llamacpp.h rewind(fp)	((void)(fseek(fp,0L,SEEK_SET),((fp)->_flag&=~_IOERR)))




// int     __cdecl unlink(const char *);
// // llamacpp.h _unlink unlink

// FILE *	__cdecl fdopen(int, const char *);
// int   __cdecl fgetchar(void);
// int   __cdecl fputchar(int);
// int	__cdecl fcloseall(void);
// long	__cdecl filesize(const char *);
// int	__cdecl flushall(void);
// int	__cdecl getch(void);
// int	__cdecl getche(void);
// int     __cdecl kbhit(void);
// char *  __cdecl tempnam (char *dir, char *pfx);
// int     __cdecl _snprintf(char *,size_t,const char *,...);
// int	__cdecl _vsnprintf(char *,size_t,const char *,va_list);
// // llamacpp.h _flushall flushall
// // llamacpp.h _fcloseall fcloseall
// // llamacpp.h _fdopen fdopen
// // llamacpp.h _tempnam tempnam
// // llamacpp.h _getche getche
// // llamacpp.h _getch getch




// wchar_t * __cdecl _wtmpnam(wchar_t *);
// FILE * __cdecl _wfopen(const wchar_t *, const wchar_t *);
// FILE * __cdecl _wfsopen(const wchar_t *, const wchar_t *, int);
// FILE * __cdecl _wfreopen(const wchar_t *, const wchar_t *, FILE *);
// wchar_t * __cdecl fgetws(wchar_t *, int, FILE *);
// int __cdecl fputws(const wchar_t *, FILE *);
// wchar_t * __cdecl _getws(wchar_t *);
// int __cdecl _putws(const wchar_t *);
// int __cdecl wprintf(const wchar_t * restrict format, ...);
// int __cdecl fwprintf(FILE * restrict stream, const wchar_t * restrict format, ...);
// int __cdecl vwprintf(const wchar_t * restrict format, va_list arg);
// int __cdecl vfwprintf(FILE * restrict stream, const wchar_t * restrict format, va_list arg);
// int __cdecl swprintf(wchar_t * restrict s, size_t n, const wchar_t * restrict format, ...);
// int __cdecl _swprintf(wchar_t * restrict s, const wchar_t * restrict format, ...);
// int __cdecl _snwprintf(wchar_t *, size_t, const wchar_t *, ...);
// int __cdecl vswprintf(wchar_t * restrict s, size_t n, const wchar_t * restrict format, va_list arg);
// int __cdecl _vswprintf(wchar_t * restrict s, const wchar_t * restrict format, va_list arg);
// int __cdecl _vsnwprintf(wchar_t *, size_t, const wchar_t *, va_list);
// int __cdecl wscanf(const wchar_t * restrict format, ...);
// int __cdecl fwscanf(FILE * restrict stream, const wchar_t * restrict format, ...);
// int __cdecl swscanf(const wchar_t * restrict s, const wchar_t * restrict format, ...);
// int __cdecl _wremove(const wchar_t *);
// void __cdecl _wperror(const wchar_t *);
// FILE * __cdecl _wfdopen(int, const wchar_t *);
// wchar_t * __cdecl _wtempnam(wchar_t *, wchar_t *);


// wint_t __cdecl fgetwc(FILE *);
// wint_t __cdecl _fgetwchar(void);
// wint_t __cdecl fputwc(wint_t, FILE *);
// wint_t __cdecl _fputwchar(wint_t);
// wint_t __cdecl getwc(FILE *);
// wint_t __cdecl getwchar(void);
// wint_t __cdecl putwc(wint_t, FILE *);
// wint_t __cdecl putwchar(wint_t);
// wint_t __cdecl ungetwc(wint_t, FILE *);
// // llamacpp.h getwchar()	fgetwc(stdin)
// // llamacpp.h putwchar(_c)	fputwc((_c),stdout)
// // llamacpp.h getwc(_stm)	fgetwc(_stm)
// // llamacpp.h putwc(_c,_stm)	fputwc(_c,_stm)








// // TODO: #ifdef porting-> llamacpp.h LLAMA_API



// // TODO: #ifdef porting-> llamacpp.h DEPRECATED(func, hint) func


// // llamacpp.h LLAMA_DEFAULT_SEED 0xFFFFFFFF

// // llamacpp.h LLAMA_MAX_RNG_STATE (64*1024)

// // llamacpp.h LLAMA_FILE_MAGIC_GGLA 0x67676c61u // 'ggla'
// // llamacpp.h LLAMA_FILE_MAGIC_GGSN 0x6767736eu // 'ggsn'

// // llamacpp.h LLAMA_SESSION_MAGIC   LLAMA_FILE_MAGIC_GGSN
// // llamacpp.h LLAMA_SESSION_VERSION 4




//     //
//     // C interface
//     //
//     // TODO: show sample usage
//     //

//     struct llama_model;
//     struct llama_context;

//     typedef int32_t llama_pos;
//     typedef int32_t llama_token;
//     typedef int32_t llama_seq_id;

//     enum llama_vocab_type {
//         LLAMA_VOCAB_TYPE_SPM = 0, // SentencePiece
//         LLAMA_VOCAB_TYPE_BPE = 1, // Byte Pair Encoding
//         LLAMA_VOCAB_TYPE_WPM = 2, // WordPiece
//     };

//     enum llama_token_type {
//         LLAMA_TOKEN_TYPE_UNDEFINED    = 0,
//         LLAMA_TOKEN_TYPE_NORMAL       = 1,
//         LLAMA_TOKEN_TYPE_UNKNOWN      = 2,
//         LLAMA_TOKEN_TYPE_CONTROL      = 3,
//         LLAMA_TOKEN_TYPE_USER_DEFINED = 4,
//         LLAMA_TOKEN_TYPE_UNUSED       = 5,
//         LLAMA_TOKEN_TYPE_BYTE         = 6,
//     };

//     // model file types
//     enum llama_ftype {
//         LLAMA_FTYPE_ALL_F32              = 0,
//         LLAMA_FTYPE_MOSTLY_F16           = 1,  // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q4_0          = 2,  // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q4_1          = 3,  // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16 = 4,  // tok_embeddings.weight and output.weight are F16
//         // LLAMA_FTYPE_MOSTLY_Q4_2       = 5,  // support has been removed
//         // LLAMA_FTYPE_MOSTLY_Q4_3       = 6,  // support has been removed
//         LLAMA_FTYPE_MOSTLY_Q8_0          = 7,  // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q5_0          = 8,  // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q5_1          = 9,  // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q2_K          = 10, // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q3_K_S        = 11, // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q3_K_M        = 12, // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q3_K_L        = 13, // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q4_K_S        = 14, // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q4_K_M        = 15, // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q5_K_S        = 16, // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q5_K_M        = 17, // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q6_K          = 18, // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_IQ2_XXS       = 19, // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_IQ2_XS        = 20, // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q2_K_S        = 21, // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_Q3_K_XS       = 22, // except 1d tensors
//         LLAMA_FTYPE_MOSTLY_IQ3_XXS       = 23, // except 1d tensors

//         LLAMA_FTYPE_GUESSED = 1024, // not specified in the model file
//     };

//     enum llama_rope_scaling_type {
//         LLAMA_ROPE_SCALING_UNSPECIFIED = -1,
//         LLAMA_ROPE_SCALING_NONE        = 0,
//         LLAMA_ROPE_SCALING_LINEAR      = 1,
//         LLAMA_ROPE_SCALING_YARN        = 2,
//         LLAMA_ROPE_SCALING_MAX_VALUE   = LLAMA_ROPE_SCALING_YARN,
//     };

//     enum llama_pooling_type {
//         LLAMA_POOLING_NONE = 0,
//         LLAMA_POOLING_MEAN = 1,
//         LLAMA_POOLING_CLS  = 2,
//     };

//     enum llama_split_mode {
//         LLAMA_SPLIT_NONE    = 0, // single GPU
//         LLAMA_SPLIT_LAYER   = 1, // split layers and KV across GPUs
//         LLAMA_SPLIT_ROW     = 2, // split rows across GPUs
//     };

//     typedef struct llama_token_data {
//         llama_token id; // token id
//         float logit;    // log-odds of the token
//         float p;        // probability of the token
//     } llama_token_data;

//     typedef struct llama_token_data_array {
//         llama_token_data * data;
//         size_t size;
//         _Bool sorted;
//     } llama_token_data_array;

//     typedef _Bool (*llama_progress_callback)(float progress, void *ctx);

//     // Input data for llama_decode
//     // A llama_batch object can contain input about one or many sequences
//     // The provided arrays (i.e. token, embd, pos, etc.) must have size of n_tokens
//     //
//     // - token  : the token ids of the input (used when embd is NULL)
//     // - embd   : token embeddings (i.e. float vector of size n_embd) (used when token is NULL)
//     // - pos    : the positions of the respective token in the sequence
//     // - seq_id : the sequence to which the respective token belongs
//     // - logits : if zero, the logits for the respective token will not be output
//     //
//     typedef struct llama_batch {
//         int32_t n_tokens;

//         llama_token  *  token;
//         float        *  embd;
//         llama_pos    *  pos;
//         int32_t      *  n_seq_id;
//         llama_seq_id ** seq_id;
//         int8_t       *  logits;

//         // NOTE: helpers for smooth API transition - can be deprecated in the future
//         //       for future-proof code, use the above fields instead and ignore everything below
//         //
//         // pos[i] = all_pos_0 + i*all_pos_1
//         //
//         llama_pos    all_pos_0;  // used if pos == NULL
//         llama_pos    all_pos_1;  // used if pos == NULL
//         llama_seq_id all_seq_id; // used if seq_id == NULL
//     } llama_batch;

//     enum llama_model_kv_override_type {
//         LLAMA_KV_OVERRIDE_INT,
//         LLAMA_KV_OVERRIDE_FLOAT,
//         LLAMA_KV_OVERRIDE_BOOL,
//     };

//     struct llama_model_kv_override {
//         char key[128];
//         enum llama_model_kv_override_type tag;
//         union {
//             int64_t int_value;
//             double float_value;
//             _Bool bool_value;
//         };
//     };

//     struct llama_model_params {
//         int32_t n_gpu_layers; // number of layers to store in VRAM
//         enum llama_split_mode split_mode; // how to split the model across multiple GPUs

//         // main_gpu interpretation depends on split_mode:
//         // LLAMA_SPLIT_NONE: the GPU that is used for the entire model
//         // LLAMA_SPLIT_ROW: the GPU that is used for small tensors and intermediate results
//         // LLAMA_SPLIT_LAYER: ignored
//         int32_t main_gpu;

//         // proportion of the model (layers or rows) to offload to each GPU, size: llama_max_devices()
//         const float * tensor_split;

//         // Called with a progress value between 0.0 and 1.0. Pass NULL to disable.
//         // If the provided progress_callback returns true, model loading continues.
//         // If it returns false, model loading is immediately aborted.
//         llama_progress_callback progress_callback;

//         // context pointer passed to the progress callback
//         void * progress_callback_user_data;

//         // override key-value pairs of the model meta data
//         const struct llama_model_kv_override * kv_overrides;

//         // Keep the booleans together to avoid misalignment during copy-by-value.
//         _Bool vocab_only; // only load the vocabulary, no weights
//         _Bool use_mmap;   // use mmap if possible
//         _Bool use_mlock;  // force system to keep model in RAM
//     };

//     struct llama_context_params {
//         uint32_t seed;              // RNG seed, -1 for random
//         uint32_t n_ctx;             // text context, 0 = from model
//         uint32_t n_batch;           // prompt processing maximum batch size
//         uint32_t n_threads;         // number of threads to use for generation
//         uint32_t n_threads_batch;   // number of threads to use for batch processing
//         int32_t  rope_scaling_type; // RoPE scaling type, from `enum llama_rope_scaling_type`

//         // ref: https://github.com/ggerganov/llama.cpp/pull/2054
//         float    rope_freq_base;   // RoPE base frequency, 0 = from model
//         float    rope_freq_scale;  // RoPE frequency scaling factor, 0 = from model
//         float    yarn_ext_factor;  // YaRN extrapolation mix factor, negative = from model
//         float    yarn_attn_factor; // YaRN magnitude scaling factor
//         float    yarn_beta_fast;   // YaRN low correction dim
//         float    yarn_beta_slow;   // YaRN high correction dim
//         uint32_t yarn_orig_ctx;    // YaRN original context size

//         ggml_backend_sched_eval_callback cb_eval;
//         void * cb_eval_user_data;

//         enum ggml_type type_k; // data type for K cache
//         enum ggml_type type_v; // data type for V cache

//         // Keep the booleans together to avoid misalignment during copy-by-value.
//         _Bool mul_mat_q;   // if true, use experimental mul_mat_q kernels (DEPRECATED - always true)
//         _Bool logits_all;  // the llama_eval() call computes all logits, not just the last one (DEPRECATED - set llama_batch.logits instead)
//         _Bool embedding;   // embedding mode only
//         _Bool offload_kqv; // whether to offload the KQV ops (including the KV cache) to GPU
//         _Bool do_pooling;  // whether to pool (sum) embedding results by sequence id (ignored if no pooling layer)
//     };

//     // model quantization parameters
//     typedef struct llama_model_quantize_params {
//         int32_t nthread;             // number of threads to use for quantizing, if <=0 will use std::thread::hardware_concurrency()
//         enum llama_ftype ftype;      // quantize to this llama_ftype
//         _Bool allow_requantize;       // allow quantizing non-f32/f16 tensors
//         _Bool quantize_output_tensor; // quantize output.weight
//         _Bool only_copy;              // only copy tensors - ftype, allow_requantize and quantize_output_tensor are ignored
//         _Bool pure;                   // disable k-quant mixtures and quantize all tensors to the same type
//         void * imatrix;              // pointer to importance matrix data
//     } llama_model_quantize_params;

//     // grammar types
//     struct llama_grammar;

//     // grammar element type
//     enum llama_gretype {
//         // end of rule definition
//         LLAMA_GRETYPE_END            = 0,

//         // start of alternate definition for rule
//         LLAMA_GRETYPE_ALT            = 1,

//         // non-terminal element: reference to rule
//         LLAMA_GRETYPE_RULE_REF       = 2,

//         // terminal element: character (code point)
//         LLAMA_GRETYPE_CHAR           = 3,

//         // inverse char(s) ([^a], [^a-b] [^abc])
//         LLAMA_GRETYPE_CHAR_NOT       = 4,

//         // modifies a preceding LLAMA_GRETYPE_CHAR or LLAMA_GRETYPE_CHAR_ALT to
//         // be an inclusive range ([a-z])
//         LLAMA_GRETYPE_CHAR_RNG_UPPER = 5,

//         // modifies a preceding LLAMA_GRETYPE_CHAR or
//         // LLAMA_GRETYPE_CHAR_RNG_UPPER to add an alternate char to match ([ab], [a-zA])
//         LLAMA_GRETYPE_CHAR_ALT       = 6,
//     };

//     typedef struct llama_grammar_element {
//         enum llama_gretype type;
//         uint32_t           value; // Unicode code point or rule ID
//     } llama_grammar_element;

//     // performance timing information
//     struct llama_timings {
//         double t_start_ms;
//         double t_end_ms;
//         double t_load_ms;
//         double t_sample_ms;
//         double t_p_eval_ms;
//         double t_eval_ms;

//         int32_t n_sample;
//         int32_t n_p_eval;
//         int32_t n_eval;
//     };

//     // Helpers for getting default parameters
//      struct llama_model_params llama_model_default_params(void);
//      struct llama_context_params llama_context_default_params(void);
//      struct llama_model_quantize_params llama_model_quantize_default_params(void);

//     // Initialize the llama + ggml backend
//     // If numa is true, use NUMA optimizations
//     // Call once at the start of the program
//      void llama_backend_init(void);

//     //optional:
//      void llama_numa_init(enum ggml_numa_strategy numa);

//     // Call once at the end of the program - currently only used for MPI
//      void llama_backend_free(void);

//      struct llama_model * llama_load_model_from_file(
//                              const char * path_model,
//             struct llama_model_params     params);

//      void llama_free_model(struct llama_model * model);

//      struct llama_context * llama_new_context_with_model(
//                      struct llama_model * model,
//             struct llama_context_params   params);

//     // Frees all allocated memory
//      void llama_free(struct llama_context * ctx);

//      int64_t llama_time_us(void);

//      size_t llama_max_devices(void);

//      _Bool llama_supports_mmap       (void);
//      _Bool llama_supports_mlock      (void);
//      _Bool llama_supports_gpu_offload(void);

//      _Bool llama_mmap_supported (void);
//      _Bool llama_mlock_supported(void);

//      const struct llama_model * llama_get_model(const struct llama_context * ctx);

//      uint32_t llama_n_ctx      (const struct llama_context * ctx);
//      uint32_t llama_n_batch    (const struct llama_context * ctx);

//      enum llama_vocab_type llama_vocab_type(const struct llama_model * model);

//      int32_t llama_n_vocab    (const struct llama_model * model);
//      int32_t llama_n_ctx_train(const struct llama_model * model);
//      int32_t llama_n_embd     (const struct llama_model * model);

//     // Get the model's RoPE frequency scaling factor
//      float llama_rope_freq_scale_train(const struct llama_model * model);

//     // Functions to access the model's GGUF metadata scalar values
//     // - The functions return the length of the string on success, or -1 on failure
//     // - The output string is always null-terminated and cleared on failure
//     // - GGUF array values are not supported by these functions

//     // Get metadata value as a string by key name
//      int32_t llama_model_meta_val_str(const struct llama_model * model, const char * key, char * buf, size_t buf_size);

//     // Get the number of metadata key/value pairs
//      int32_t llama_model_meta_count(const struct llama_model * model);

//     // Get metadata key name by index
//      int32_t llama_model_meta_key_by_index(const struct llama_model * model, int32_t i, char * buf, size_t buf_size);

//     // Get metadata value as a string by index
//      int32_t llama_model_meta_val_str_by_index(const struct llama_model * model, int32_t i, char * buf, size_t buf_size);

//     // Get a string describing the model type
//      int32_t llama_model_desc(const struct llama_model * model, char * buf, size_t buf_size);

//     // Returns the total size of all the tensors in the model in bytes
//      uint64_t llama_model_size(const struct llama_model * model);

//     // Returns the total number of parameters in the model
//      uint64_t llama_model_n_params(const struct llama_model * model);

//     // Get a llama model tensor
//      struct ggml_tensor * llama_get_model_tensor(struct llama_model * model, const char * name);

//     // Returns 0 on success
//      uint32_t llama_model_quantize(
//             const char * fname_inp,
//             const char * fname_out,
//             const llama_model_quantize_params * params);

//     // Apply a LoRA adapter to a loaded model
//     // path_base_model is the path to a higher quality model to use as a base for
//     // the layers modified by the adapter. Can be NULL to use the current loaded model.
//     // The model needs to be reloaded before applying a new adapter, otherwise the adapter
//     // will be applied on top of the previous one
//     // Returns 0 on success
//      int32_t llama_apply_lora_from_file( struct llama_context * ctx, const char * path_lora, float scale, const char * path_base_model, int32_t n_threads);

//      int32_t llama_model_apply_lora_from_file(
//             const struct llama_model * model,
//                       const char * path_lora,
//                            float   scale,
//                       const char * path_base_model,
//                          int32_t   n_threads);

//     //
//     // KV cache
//     //

//     // Information associated with an individual cell in the KV cache view.
//     struct llama_kv_cache_view_cell {
//         // The position for this cell. Takes KV cache shifts into account.
//         // May be negative if the cell is not populated.
//         llama_pos pos;
//     };

//     // An updateable view of the KV cache.
//     struct llama_kv_cache_view {
//         // Number of KV cache cells. This will be the same as the context size.
//         int32_t n_cells;

//         // Maximum number of sequences that can exist in a cell. It's not an error
//         // if there are more sequences in a cell than this value, however they will
//         // not be visible in the view cells_sequences.
//         int32_t n_max_seq;

//         // Number of tokens in the cache. For example, if there are two populated
//         // cells, the first with 1 sequence id in it and the second with 2 sequence
//         // ids then you'll have 3 tokens.
//         int32_t token_count;

//         // Number of populated cache cells.
//         int32_t used_cells;

//         // Maximum contiguous empty slots in the cache.
//         int32_t max_contiguous;

//         // Index to the start of the max_contiguous slot range. Can be negative
//         // when cache is full.
//         int32_t max_contiguous_idx;

//         // Information for an individual cell.
//         struct llama_kv_cache_view_cell * cells;

//         // The sequences for each cell. There will be n_max_seq items per cell.
//         llama_seq_id * cells_sequences;
//     };

//     // Create an empty KV cache view. (use only for debugging purposes)
//      struct llama_kv_cache_view llama_kv_cache_view_init(const struct llama_context * ctx, int32_t n_max_seq);

//     // Free a KV cache view. (use only for debugging purposes)
//      void llama_kv_cache_view_free(struct llama_kv_cache_view * view);

//     // Update the KV cache view structure with the current state of the KV cache. (use only for debugging purposes)
//      void llama_kv_cache_view_update(const struct llama_context * ctx, struct llama_kv_cache_view * view);

//     // Returns the number of tokens in the KV cache (slow, use only for debug)
//     // If a KV cell has multiple sequences assigned to it, it will be counted multiple times
//      int32_t llama_get_kv_cache_token_count(const struct llama_context * ctx);

//     // Returns the number of used KV cells (i.e. have at least one sequence assigned to them)
//      int32_t llama_get_kv_cache_used_cells(const struct llama_context * ctx);

//     // Clear the KV cache
//      void llama_kv_cache_clear(
//             struct llama_context * ctx);

//     // Removes all tokens that belong to the specified sequence and have positions in [p0, p1)
//     // seq_id < 0 : match any sequence
//     // p0 < 0     : [0,  p1]
//     // p1 < 0     : [p0, inf)
//      void llama_kv_cache_seq_rm(
//             struct llama_context * ctx,
//                     llama_seq_id   seq_id,
//                        llama_pos   p0,
//                        llama_pos   p1);

//     // Copy all tokens that belong to the specified sequence to another sequence
//     // Note that this does not allocate extra KV cache memory - it simply assigns the tokens to the new sequence
//     // p0 < 0 : [0,  p1]
//     // p1 < 0 : [p0, inf)
//      void llama_kv_cache_seq_cp(
//             struct llama_context * ctx,
//                     llama_seq_id   seq_id_src,
//                     llama_seq_id   seq_id_dst,
//                        llama_pos   p0,
//                        llama_pos   p1);

//     // Removes all tokens that do not belong to the specified sequence
//      void llama_kv_cache_seq_keep(
//             struct llama_context * ctx,
//                     llama_seq_id   seq_id);

//     // Adds relative position "delta" to all tokens that belong to the specified sequence and have positions in [p0, p1)
//     // If the KV cache is RoPEd, the KV data is updated accordingly
//     // p0 < 0 : [0,  p1]
//     // p1 < 0 : [p0, inf)
//      void llama_kv_cache_seq_shift(
//             struct llama_context * ctx,
//                     llama_seq_id   seq_id,
//                        llama_pos   p0,
//                        llama_pos   p1,
//                        llama_pos   delta);

//     // Integer division of the positions by factor of `d > 1`
//     // If the KV cache is RoPEd, the KV data is updated accordingly
//     // p0 < 0 : [0,  p1]
//     // p1 < 0 : [p0, inf)
//      void llama_kv_cache_seq_div(
//             struct llama_context * ctx,
//                     llama_seq_id   seq_id,
//                        llama_pos   p0,
//                        llama_pos   p1,
//                              int   d);

//     //
//     // State / sessions
//     //

//     // Returns the maximum size in bytes of the state (rng, logits, embedding
//     // and kv_cache) - will often be smaller after compacting tokens
//      size_t llama_get_state_size(const struct llama_context * ctx);

//     // Copies the state to the specified destination address.
//     // Destination needs to have allocated enough memory.
//     // Returns the number of bytes copied
//      size_t llama_copy_state_data(
//             struct llama_context * ctx,
//                          uint8_t * dst);

//     // Set the state reading from the specified address
//     // Returns the number of bytes read
//      size_t llama_set_state_data(
//             struct llama_context * ctx,
//                          uint8_t * src);

//     // Save/load session file
//      _Bool llama_load_session_file(
//             struct llama_context * ctx,
//                       const char * path_session,
//                      llama_token * tokens_out,
//                           size_t   n_token_capacity,
//                           size_t * n_token_count_out);

//      _Bool llama_save_session_file(
//             struct llama_context * ctx,
//                       const char * path_session,
//                const llama_token * tokens,
//                           size_t   n_token_count);

//     //
//     // Decoding
//     //

//     // Run the llama inference to obtain the logits and probabilities for the next token(s).
//     // tokens + n_tokens is the provided batch of new tokens to process
//     // n_past is the number of tokens to use from previous eval calls
//     // Returns 0 on success
//     // DEPRECATED: use llama_decode() instead
//      int llama_eval( struct llama_context * ctx, llama_token * tokens, int32_t n_tokens, int32_t n_past);

//     // Same as llama_eval, but use float matrix input directly.
//     // DEPRECATED: use llama_decode() instead
//      int llama_eval_embd( struct llama_context * ctx, float * embd, int32_t n_tokens, int32_t n_past);

//     // Return batch for single sequence of tokens starting at pos_0
//     //
//     // NOTE: this is a helper function to facilitate transition to the new batch API - avoid using it
//     //
//      struct llama_batch llama_batch_get_one(
//                   llama_token * tokens,
//                       int32_t   n_tokens,
//                     llama_pos   pos_0,
//                  llama_seq_id   seq_id);

//     // Allocates a batch of tokens on the heap that can hold a maximum of n_tokens
//     // Each token can be assigned up to n_seq_max sequence ids
//     // The batch has to be freed with llama_batch_free()
//     // If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)
//     // Otherwise, llama_batch.token will be allocated to store n_tokens llama_token
//     // The rest of the llama_batch members are allocated with size n_tokens
//     // All members are left uninitialized
//      struct llama_batch llama_batch_init(
//             int32_t n_tokens,
//             int32_t embd,
//             int32_t n_seq_max);

//     // Frees a batch of tokens allocated with llama_batch_init()
//      void llama_batch_free(struct llama_batch batch);

//     // Positive return values does not mean a fatal error, but rather a warning.
//     //   0 - success
//     //   1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)
//     // < 0 - error
//      int32_t llama_decode(
//             struct llama_context * ctx,
//               struct llama_batch   batch);

//     // Set the number of threads used for decoding
//     // n_threads is the number of threads used for generation (single token)
//     // n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)
//      void llama_set_n_threads(struct llama_context * ctx, uint32_t n_threads, uint32_t n_threads_batch);

//     // Token logits obtained from the last call to llama_eval()
//     // The logits for the last token are stored in the last row
//     // Logits for which llama_batch.logits[i] == 0 are undefined
//     // Rows: n_tokens provided with llama_batch
//     // Cols: n_vocab
//      float * llama_get_logits(struct llama_context * ctx);

//     // Logits for the ith token. Equivalent to:
//     // llama_get_logits(ctx) + i*n_vocab
//      float * llama_get_logits_ith(struct llama_context * ctx, int32_t i);

//     // Get the embeddings for the input
//     // shape: [n_embd] (1-dimensional)
//      float * llama_get_embeddings(struct llama_context * ctx);

//     // Get the embeddings for the ith sequence
//     // llama_get_embeddings(ctx) + i*n_embd
//      float * llama_get_embeddings_ith(struct llama_context * ctx, int32_t i);

//     //
//     // Vocab
//     //

//      const char * llama_token_get_text(const struct llama_model * model, llama_token token);

//      float llama_token_get_score(const struct llama_model * model, llama_token token);

//      enum llama_token_type llama_token_get_type(const struct llama_model * model, llama_token token);

//     // Special tokens
//      llama_token llama_token_bos(const struct llama_model * model); // beginning-of-sentence
//      llama_token llama_token_eos(const struct llama_model * model); // end-of-sentence
//      llama_token llama_token_nl (const struct llama_model * model); // next-line

//     // Returns -1 if unknown, 1 for true or 0 for false.
//      int32_t         llama_add_bos_token(const struct llama_model * model);

//     // Returns -1 if unknown, 1 for true or 0 for false.
//      int32_t         llama_add_eos_token(const struct llama_model * model);

//     // codellama infill tokens
//      llama_token llama_token_prefix(const struct llama_model * model); // Beginning of infill prefix
//      llama_token llama_token_middle(const struct llama_model * model); // Beginning of infill middle
//      llama_token llama_token_suffix(const struct llama_model * model); // Beginning of infill suffix
//      llama_token llama_token_eot   (const struct llama_model * model); // End of infill middle

//     //
//     // Tokenization
//     //

//     /// @details Convert the provided text into tokens.
//     /// @param tokens The tokens pointer must be large enough to hold the resulting tokens.
//     /// @return Returns the number of tokens on success, no more than n_max_tokens
//     /// @return Returns a negative number on failure - the number of tokens that would have been returned
//     /// @param special Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext.
//     ///                Does not insert a leading space.
//      int32_t llama_tokenize(
//         const struct llama_model * model,
//                       const char * text,
//                          int32_t   text_len,
//                      llama_token * tokens,
//                          int32_t   n_max_tokens,
//                             _Bool   add_bos,
//                             _Bool   special);

//     // Token Id -> Piece.
//     // Uses the vocabulary in the provided context.
//     // Does not write null terminator to the buffer.
//     // User code is responsible to remove the leading whitespace of the first non-BOS token when decoding multiple tokens.
//      int32_t llama_token_to_piece(
//               const struct llama_model * model,
//                            llama_token   token,
//                                   char * buf,
//                                int32_t   length);

//     //
//     // Grammar
//     //

//      struct llama_grammar * llama_grammar_init(
//             const llama_grammar_element ** rules,
//                                  size_t    n_rules,
//                                  size_t    start_rule_index);

//      void llama_grammar_free(struct llama_grammar * grammar);

//      struct llama_grammar * llama_grammar_copy(const struct llama_grammar * grammar);

//     //
//     // Sampling functions
//     //

//     // Sets the current rng seed.
//      void llama_set_rng_seed(struct llama_context * ctx, uint32_t seed);

//     /// @details Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix.
//     /// @details Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.
//      void llama_sample_repetition_penalties(
//             struct llama_context * ctx,
//           llama_token_data_array * candidates,
//                const llama_token * last_tokens,
//                           size_t   penalty_last_n,
//                            float   penalty_repeat,
//                            float   penalty_freq,
//                            float   penalty_present);

//     /// @details Apply classifier-free guidance to the logits as described in academic paper "Stay on topic with Classifier-Free Guidance" https://arxiv.org/abs/2306.17806
//     /// @param logits Logits extracted from the original generation context.
//     /// @param logits_guidance Logits extracted from a separate context from the same model. Other than a negative prompt at the beginning, it should have all generated and user input tokens copied from the main context.
//     /// @param scale Guidance strength. 1.0f means no guidance. Higher values mean stronger guidance.
//      void llama_sample_apply_guidance(
//               struct llama_context * ctx,
//                              float * logits,
//                              float * logits_guidance,
//                              float   scale);

//      void llama_sample_classifier_free_guidance( struct llama_context * ctx, llama_token_data_array * candidates, struct llama_context * guidance_ctx, float scale);

//     /// @details Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.
//      void llama_sample_softmax(
//             struct llama_context * ctx,
//           llama_token_data_array * candidates);

//     /// @details Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
//      void llama_sample_top_k(
//             struct llama_context * ctx,
//           llama_token_data_array * candidates,
//                          int32_t   k,
//                           size_t   min_keep);

//     /// @details Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
//      void llama_sample_top_p(
//             struct llama_context * ctx,
//           llama_token_data_array * candidates,
//                            float   p,
//                           size_t   min_keep);

//     /// @details Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841
//      void llama_sample_min_p(
//             struct llama_context * ctx,
//           llama_token_data_array * candidates,
//                            float   p,
//                           size_t   min_keep);

//     /// @details Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.
//      void llama_sample_tail_free(
//             struct llama_context * ctx,
//           llama_token_data_array * candidates,
//                            float   z,
//                           size_t   min_keep);

//     /// @details Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.
//      void llama_sample_typical(
//             struct llama_context * ctx,
//           llama_token_data_array * candidates,
//                            float   p,
//                           size_t   min_keep);

//     /// @details Dynamic temperature implementation described in the paper https://arxiv.org/abs/2309.02772.
//      void llama_sample_entropy(
//             struct llama_context * ctx,
//           llama_token_data_array * candidates_p,
//                            float   min_temp,
//                            float   max_temp,
//                            float   exponent_val);

//      void llama_sample_temp(
//             struct llama_context * ctx,
//           llama_token_data_array * candidates,
//                            float   temp);

//      void llama_sample_temperature( struct llama_context * ctx, llama_token_data_array * candidates, float temp);

//     /// @details Apply constraints from grammar
//      void llama_sample_grammar(
//             struct llama_context * ctx,
//           llama_token_data_array * candidates,
//       const struct llama_grammar * grammar);

//     /// @details Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
//     /// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
//     /// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
//     /// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
//     /// @param m The number of tokens considered in the estimation of `s_hat`. This is an arbitrary value that is used to calculate `s_hat`, which in turn helps to calculate the value of `k`. In the paper, they use `m = 100`, but you can experiment with different values to see how it affects the performance of the algorithm.
//     /// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
//      llama_token llama_sample_token_mirostat(
//             struct llama_context * ctx,
//           llama_token_data_array * candidates,
//                            float   tau,
//                            float   eta,
//                          int32_t   m,
//                            float * mu);

//     /// @details Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
//     /// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
//     /// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
//     /// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
//     /// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
//      llama_token llama_sample_token_mirostat_v2(
//             struct llama_context * ctx,
//           llama_token_data_array * candidates,
//                            float   tau,
//                            float   eta,
//                            float * mu);

//     /// @details Selects the token with the highest probability.
//     ///          Does not compute the token probabilities. Use llama_sample_softmax() instead.
//      llama_token llama_sample_token_greedy(
//             struct llama_context * ctx,
//           llama_token_data_array * candidates);

//     /// @details Randomly selects a token from the candidates based on their probabilities.
//      llama_token llama_sample_token(
//             struct llama_context * ctx,
//           llama_token_data_array * candidates);

//     /// @details Accepts the sampled token into the grammar
//      void llama_grammar_accept_token(
//             struct llama_context * ctx,
//             struct llama_grammar * grammar,
//                      llama_token   token);

//     //
//     // Beam search
//     //

//     struct llama_beam_view {
//         const llama_token * tokens;

//         size_t n_tokens;
//         float  p;        // Cumulative beam probability (renormalized relative to all beams)
//         _Bool   eob;      // Callback should set this to true when a beam is at end-of-beam.
//     };

//     // Passed to beam_search_callback function.
//     // Whenever 0 < common_prefix_length, this number of tokens should be copied from any of the beams
//     // (e.g. beams[0]) as they will be removed (shifted) from all beams in all subsequent callbacks.
//     // These pointers are valid only during the synchronous callback, so should not be saved.
//     struct llama_beams_state {
//         struct llama_beam_view * beam_views;

//         size_t n_beams;               // Number of elements in beam_views[].
//         size_t common_prefix_length;  // Current max length of prefix tokens shared by all beams.
//         _Bool   last_call;             // True iff this is the last callback invocation.
//     };

//     // Type of pointer to the beam_search_callback function.
//     // void* callback_data is any custom data passed to llama_beam_search, that is subsequently
//     // passed back to beam_search_callback. This avoids having to use global variables in the callback.
//     typedef void (*llama_beam_search_callback_fn_t)(void * callback_data, struct llama_beams_state);

//     /// @details Deterministically returns entire sentence constructed by a beam search.
//     /// @param ctx Pointer to the llama_context.
//     /// @param callback Invoked for each iteration of the beam_search loop, passing in beams_state.
//     /// @param callback_data A pointer that is simply passed back to callback.
//     /// @param n_beams Number of beams to use.
//     /// @param n_past Number of tokens already evaluated.
//     /// @param n_predict Maximum number of tokens to predict. EOS may occur earlier.
//      void llama_beam_search(
//                    struct llama_context * ctx,
//         llama_beam_search_callback_fn_t   callback,
//                                    void * callback_data,
//                                  size_t   n_beams,
//                                 int32_t   n_past,
//                                 int32_t   n_predict);

//     // Performance information
//      struct llama_timings llama_get_timings(struct llama_context * ctx);

//      void llama_print_timings(struct llama_context * ctx);
//      void llama_reset_timings(struct llama_context * ctx);

//     // Print system information
//      const char * llama_print_system_info(void);

//     // Set callback for all future logging events.
//     // If this is not called, or NULL is supplied, everything is output on stderr.
//      void llama_log_set(ggml_log_callback log_callback, void * user_data);

//      void llama_dump_timing_info_yaml(FILE * stream, const struct llama_context * ctx);




// Internal API to be implemented by llama.cpp and used by tests/benchmarks only



